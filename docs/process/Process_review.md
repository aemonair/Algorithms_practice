[TOC]
# 一、概念
## 1.1. 进程
### 1.1.0 进程概念
> 运行中的程序，就被称为「进程」
- 进程是程序执行时的一个实例，是系统进行**资源分配的基本单位**。
  * 所有与该进程有关的资源，都被记录在进程控制块(PCB)中。以表示该进程拥有这些资源或正在使用它们。
  * 另外，进程也是抢占处理机的调度单位，它拥有一个完整的虚拟地址空间。当进程发生调度时，不同的进程拥有不同的虚拟地址空间，而同一进程内的不同线程共享同一地址空间。
```cpp
  struct task_struct {
    // 进程状态
    long              state;
    // 虚拟内存结构体
    struct mm_struct  *mm;
    // 进程号
    pid_t             pid;
    // 指向父进程的指针
    struct task_struct   *parent;
    // 子进程列表
    struct list_head      children;
    // 存放文件系统信息的指针
    struct fs_struct      *fs;
    // 一个数组，包含该进程打开的文件指针
    struct files_struct   *files;
};
```
#### 1.1.1 files文件描述符
0 是输入，1 是输出，2 是错误。

### 1.1.2 进程执行
程序是指令、数据及其组织形式的描述，进程是程序的实体。程序本身是没有生命周期的，它只是存在磁盘上的一些指令,程序一旦运行就是进程。
当程序需要运行时，操作系统将代码和所有静态数据记载到内存和进程的地址空间（每个进程都拥有唯一的地址空间，见下图所示）中，
通过创建和初始化栈（局部变量，函数参数和返回地址)、
分配堆内存以及与IO相关的任务，
当前期准备工作完成，
启动程序，OS将CPU的控制权转移到新创建的进程，
进程开始运行。

### 1.1.3 PCB
操作系统对进程的控制和管理通过`PCB(Processing Control Block)`，PCB通常是系统内存占用区中的一个连续存区，
它存放着操作系统用于描述进程情况及控制进程运行所需的全部信息(进程标识号,进程状态,进程优先级,文件系统指针以及各个寄存器的内容等)，
进程的PCB是系统感知进程的唯一实体。

#### 1.1.3.1 <span style="color:red;">进程描述信息</span>：
- 进程标识符`pid_t pid;`：标识各个进程，每个进程都有一个并且唯一的标识符；
- 用户标识符：进程归属的用户，用户标识符主要为共享和保护服务；

#### 1.1.3.2 <span style="color:red;">进程控制和管理信息</span>：
- 进程当前状态`long state;`，如 new、ready、running、waiting 或 blocked 等；
- 进程优先级：进程抢占 CPU 时的优先级；

#### 1.1.3.3 <span style="color:red;">资源分配清单</span>：
- 有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I/O 设备信息。
`struct files_struct   *files;`

#### 1.1.3.4 <span style="color:red;">CPU 相关信息</span>：
- CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程重新执行时，能从断点处继续执行。

### 1.1.3. 每个 PCB 是如何组织的呢？
通常是通过`链表`的方式进行组织，把具有相同状态的进程链在一起，组成各种队列。(阻塞队列/就绪队列)
除了链接的组织方式，还有`索引`方式，它的工作原理：将同一状态的进程组织在一个`索引表`中，索引表项指向相应的 PCB，不同状态对应不同的索引表。
一般会选择链表，因为可能面临进程创建，销毁等调度导致进程状态发生变化，所以链表能够更加灵活的插入和删除。

### 1.1.4 进程执行状态
一个进程至少具有5种基本状态：
 初始态、执行状态、等待（阻塞）状态、就绪状态、终止状态

#### 1.1.4.1 初始状态：
-  进程刚被创建，由于其他进程正占有CPU所以得不到执行，只能处于初始状态。
#### 1.1.4.2 **执行状态Running**：
- 任意时刻处于执行状态的进程只能有一个。该时刻进程占用 CPU。
#### 1.1.4.3 **就绪状态Ready**：
- 只有处于就绪状态的经过调度才能到执行状态。可运行，但因为其他进程正在运行而暂停停止；
#### 1.1.4.4 **等待阻塞状态Blocked**：
- 进程等待某件事件完成，阻塞。该进程正在等待某一事件发生（如等待输入/输出操作的完成）而暂时停止运行，这时，即使给它CPU控制权，它也无法运行；
#### 1.1.4.5 停止结束状态Exit：
- 进程结束，进程正在从系统中消失时的状态；

#### 1.1.4.6 状态变迁：
`NULL     -> 创建状态`：一个新进程被创建时的第一个状态；
`创建状态 -> 就绪状态`：当进程被创建完成并初始化后，一切就绪准备运行时，变为就绪状态，这个过程是很快的；
`就绪状态 -> 运行状态`：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给 CPU 正式运行该进程；
`运行状态 -> 结束状态`：当进程已经运行完成或出错时，会被操作系统作结束状态处理；
`运行状态 -> 就绪状态`：处于运行状态的进程在运行过程中，由于分配给它的运行时间片用完，操作系统会把该进程变为就绪态，接着从就绪态选中另外一个进程运行；
`运行状态 -> 阻塞状态`：当进程请求某个事件且必须等待时，例如请求 I/O 事件；
`阻塞状态 -> 就绪状态`：当进程要等待的事件完成时，它从阻塞状态变到就绪状态；

#### 1.1.4.6 挂起
还有一个状态叫挂起状态，它表示进程没有占有内存空间。这跟阻塞状态是不一样，阻塞状态是等待某个事件的返回。
挂起状态可以分为两种：

- 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现；
- 就绪挂起状态：进程在外存（硬盘），但只要进入内存，即刻立刻运行；
这两种挂起状态加上前面的五种状态，就变成了七种状态变迁，
![进程七种状态变迁
](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZcvw4t9kicec370n3cvX2JS9OSw0O4hBZhsvyrPTCkXqwCg9QgtBfdrCsU90NaspiabyILN5QxmAYxQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1)

### 1.1.5 进程切换
操作系统对把CPU控制权在不同进程之间交换执行的机制成为<span style="color:red;">`上下文切换`（context switch）</span>，
即保存当前进程的上下文，恢复新进程的上下文，然后将CPU控制权转移到新进程，新进程就会从上次停止的地方开始。
因此，进程是轮流使用CPU的，CPU被若干进程共享，使用某种调度算法来决定何时停止一个进程，并转而为另一个进程提供服务
-单核CPU双进程的情况 ：

 - 进程直接特定的机制和遇到I/O中断的情况下，进行上下文切换，轮流使用CPU资源
- 双核CPU双进程的情况 ：
 - 每一个进程独占一个CPU核心资源，在处理I/O请求的时候，CPU处于阻塞状态

### 1.1.6 虚拟存储 进程间数据共享
系统提供了一种对主存的抽象概念，即为虚拟存储器（VM）。
虚拟存储（Virtual Memory） 是一种内存管理技术，它是一个抽象的概念，它为每一个进程提供了一个假象，即每个进程都在独占地使用主存。
它使得每个进程都认为自己在使用全部的（或大部分的）物理内存，但实际上，进程所使用的内存可能只是物理内存的一小部分，甚至可能被交换出到磁盘上。
虚拟存储系统允许我们把物理内存看作是内存池的一个缓存（cache），它缓存的是在磁盘上存储的数据和代码。每个进程有自己的虚拟地址空间，这个地址空间被分割成多个固定大小的区块，称为页面（page）。对应地，物理内存也被分割成同样大小的区块，称为页框（page frame）。
虚拟存储器主要提供了三个能力：

- 将主存看成是一个存储在磁盘上的<font color='red'>高速缓存</font>，在主存中只保存活动区域，并根据需要在磁盘和主存之间来回传送数据，通过这种方式，更高效地使用主存
- 为每个进程提供了一致的地址空间，从而<font color='red'>简化了存储器管理</font>
- 保护了每个进程的地址空间<font color='red'>不被其他进程破坏</font>
> 由于进程拥有自己独占的虚拟地址空间，CPU通过地址翻译将虚拟地址转换成真实的物理地址，每个进程只能访问自己的地址空间。
> 因此，在没有其他机制（进程间通信）的辅助下，进程之间是无法共享数据的

在虚拟存储系统中，进程间数据共享通常是通过映射同一块物理内存到不同进程的虚拟地址空间来实现的。例如，在Unix系统中，可以使用mmap系统调用创建一块共享内存区域。

进程间数据共享（Inter-Process Communication, IPC） 是操作系统提供的一种机制，使得在运行在同一台机器上的不同进程可以相互交换数据。IPC机制有多种实现方式，包括管道（pipe），消息队列（message queue），信号（signal），共享内存（shared memory）等。
## 1.2. 线程
### 1.2.0 线程
-  线程，有时也被称为轻量级进程，是程序执行流的最小单元，是进程中的一个实体，是被**系统独立调度和分派的基本单位**。
  * 与进程不同，线程与资源分配无关，线程自己不拥有系统资源，它属于某一个进程，并与进程内的其他线程一起共享进程的资源。
  * 线程只由相关堆栈（系统栈或用户栈）寄存器和线程控制表TCB组成。

一个进程可以有一个或多个线程，
同一进程中的多个线程将共享该进程中的全部系统资源，如虚拟地址空间，文件描述符和信号处理等等。
但同一进程中的多个线程有各自的调用栈和线程本地存储。

### 1.2.1 TCB线程控制块
系统利用PCB来完成对进程的控制和管理。
同样，系统为线程分配一个线程控制块TCB（Thread Control Block）,将所有用于控制和管理线程的信息记录在线程的控制块中，
TCB中通常包括：

- 线程标志符
- 一组寄存器
- 线程运行状态
- 优先级
- 线程专有存储区
- 信号屏蔽

fork创建进程，pthread创建线程
但无论线程还是进程，都是用task_struct结构表示的，唯一的区别就是共享的数据区域不同。
线程共享 虚拟内存、文件描述符；
mm指向的是进程的虚拟内存，也就是载入资源和可执行文件的地方；files指针指向一个数组，这个数组里装着所有该进程打开的文件的指针。

## 1.3. 进程和线程的关系
### 1.3.0 进程和线程的关系
- 通常在一个进程中可以包含若干个线程，它们可以利用进程所拥有的资源。
- 但是，一个线程只属于一个进程。进程间相互独立，同一进程的各线程间共享。某进程内的线程在其它进程不可见。而且需要注意的是，线程不是一个可执行的实体。
### 1.4.0 进程和线程的比较
#### 1.4.1 调度 ：
在引入线程的操作系统中，线程是调度和分配的基本单位 ，进程是资源拥有的基本单位 。
把传统进程的两个属性分开，线程便能轻装运行，从而可 显著地提高系统的并发程度 。
在同一进程中，线程的切换不会引起进程的切换；在由一个进程中的线程切换到另一个进程中的线程时，才会引起进程的切换。

#### 1.4.2 并发性 ：
在引入线程的操作系统中，不仅进程之间可以并发执行，而且在一个进程中的多个线程之间亦可并发执行，因而使操作系统具有更好的并发性，从而能 更有效地使用系统资源和提高系统吞吐量。
#### 1.4.3 拥有资源 ：
不论是传统的操作系统，还是设有线程的操作系统，进程都是拥有资源的一个独立 单位，它可以拥有自己的资源。一般地说，线程自己不拥有系统资源（只有一些必不可少的资源，但它可以访问其隶属进程的资源。
#### 1.4.4 系统开销：
由于在创建或撤消进程时，系统都要为之分配或回收资源，因此，操作系统所付出的开销将显著地大于在创建或撤消线程时的开销。进程切换的开销也远大于线程切换的开销。
由于在创建或撤消进程时，系统都要为之分配或回收资源，如内存空间、I／o设备等。因此，操作系统所付出的开销将显著地大于在创建或撤消线程时的开销。类似地，在进行进程切换时，涉及到整个当前进程CPU环境的保存以及新被调度运行的进程的CPU环境的设置。而线程切换只须保存和设置少量寄存器的内容，并不涉及存储器管理方面的操作。可见，进程切换的开销也远大于线程切换的开销。
#### 1.4.5 通信：
由于同一进程中的多个线程具有相同的地址空间，致使它们之间的同步和通信的实现，也变得比较容易。
进程间通信IPC，线程间可以直接读写进程数据段（如全局变量）来进行通信——需要进程同步和互斥手段的辅助，以保证数据的一致性，因此共享简单。但是线程的数据同步要比进程略复杂。
#### 1.4.6 创建及调试
进程编程调试简单可靠性高，但是创建销毁开销大；线程正相反，开销小，切换速度快，但是编程调试相对复杂
#### 1.4.7 互相影响
进程间不会相互影响 ；线程一个线程挂掉将导致整个进程挂掉
#### 1.4.8 多核
进程适应于多核、多机分布；线程适用于多核



### 1.5.0 进程和线程区别？
1. 进程是资源分配的基本单位，线程是cpu调度，或者说是程序执行的最小单位。但是并不是说CPU不在以进程为单位进行调度，虽然在某些操作系统中是这样。同一个进程中并行运行多个线程，就是对在同一台计算机上运行多个进程的模拟。
2. 进程有独立的地址空间，而同一进程中的线程共享该进程的地址空间。比如在linux下面启动一个新的进程，系统必须分配给它独立的地址空间，建立众多的数据表来维护它的代码段、堆栈段和数据段，这是一种非常昂贵的多任务工作方式。而运行一个进程中的线程，它们之间共享大部分数据，使用相同的地址空间，因此启动一个线程，切换一个线程远比进程操作要快，花费也要小得多。当然，线程是拥有自己的寄存器和堆栈（线程栈），比如在windows中用_beginthreadex创建一个新进程就会在调用CreateThread的同时申请一个专属于线程的数据块（_tiddata)。虽然，线程有自己线程栈，线程可以直接访问全局变量，甚至可以访问进程地址空间中的每一个内存，所以一个线程可以读写甚至清楚另一个线程的堆栈。
3. 线程之间的通信比较方便。统一进程下的线程共享数据（比如全局变量，静态变量，打开的文件，子进程），通过这些数据来通信不仅快捷而且方便，当然如何处理好这些访问的同步与互斥正是编写多线程程序的难点。而进程之间的通信只能通过进程通信的方式进行。在一个线程中分配的堆在各个线程中均可以使用，在一个线程中打开的文件各个线程均可用，当然指同一进程中的线程。
4. 多进程比多线程程序要健壮。一个线程死掉整个进程就死掉了，但是在保护模式下，一个进程死掉对另一个进程没有直接影响。
5. 线程的执行与进程是有区别的。每个独立的线程有有自己的一个程序入口，顺序执行序列和程序的出口，但是线程不能独立执行，必须依附与程序之中，由应用程序提供多个线程的并发控制。
6. linux中进程具有父子关系，形成进程树，但是线程是平等的没有父子关系
### 1.5.1 什么时候用多进程多线程?
#### 1.5.1.1 适合使用多进程的情况：

1.CPU密集型任务：当任务主要涉及到大量的计算和CPU运算时，使用多进程可以将任务分配给多个核心进行并行计算，充分利用多核处理器的优势。
2.独立性要求高：如果任务之间相互独立，不需要频繁的数据共享和通信，那么可以选择多进程，每个进程都有独立的内存空间，避免了数据共享的一些问题。

#### 1.5.1.2 适合使用多线程的情况：
1.I/O密集型任务：当任务主要涉及到等待I/O操作（如网络请求、文件读写）时，使用多线程可以提高程序的响应性能。因为在一个线程等待I/O时，其他线程可以继续执行，充分利用了等待时间。
2.资源共享和通信：如果多个任务之间需要频繁的数据共享和通信，使用多线程可以更方便地实现。线程共享同一个进程的内存空间，数据可以直接传递和共享，避免了进程间通信的开销和复杂性。

#### 1.5.1.3 还有一些情况可以考虑综合使用多进程和多线程：
1.复杂的应用程序：对于复杂的应用程序，可以根据任务的性质灵活地使用多进程和多线程。例如，可以将主要的计算任务放在多进程中进行并行计算，而将I/O操作放在多线程中处理，实现计算和I/O并行。
2.可扩展性要求高：如果应用程序需要应对大规模的并发请求，而且计算和I/O任务都较为繁重，可以考虑综合使用多进程和多线程来满足可扩展性和性能的需求。
总的来说，选择使用多进程还是多线程需要综合考虑任务类型、性能需求、数据共享和通信等因素。没有绝对的规则，根据具体情况和需求进行选择，权衡各种因素来获得最佳的性能和并发效果。
### 1.5.2 进程创建多少个线程?
进程能够创建的线程数量取决于多个因素，包括操作系统、硬件限制、可用系统资源（如内存）、以及操作系统的配置等。下面是一些影响线程数量的主要因素：

#### 1.5.2.1 操作系统限制
- **Windows**：在Windows系统中，每个线程默认占用1 MB的栈空间（64位系统上更多）。因此，理论上进程可以创建的线程数量受到可用虚拟地址空间的限制。例如，在32位系统上，一个进程的地址空间通常是2 GB（或4 GB，如果启用了/3GB开关），因此最多可以创建大约2000个线程。
- **Linux**：在Linux系统中，线程的数量主要受限于系统资源（如内存）和`ulimit`设置。可以通过`ulimit -u`查看用户可以创建的最大进程数（包括线程），通过`/proc/sys/kernel/threads-max`查看系统级别的最大线程数。

#### 1.5.2.2 硬件限制
- **内存**：每个线程都需要一定量的内存来存储其栈空间。如果系统内存不足，将无法创建更多的线程。
- **CPU核心数**：虽然CPU核心数不是直接限制线程数量的因素，但它影响了并行执行线程的能力。过多的线程可能导致频繁的上下文切换，从而降低整体性能。

#### 1.5.2.3 应用程序设计
- **资源消耗**：除了内存外，线程还可能消耗其他资源，如文件描述符、网络连接等。如果这些资源耗尽，也将无法创建新的线程。
- **性能考虑**：过多的线程会导致大量的上下文切换开销，反而可能降低应用程序的性能。因此，合理的线程池大小设计是非常重要的。

#### 1.5.2.4 如何决定线程数量
1. **基准测试**：通过基准测试来确定最优的线程数量。可以在不同的线程数量下运行应用程序，观察性能指标，如吞吐量、响应时间和资源利用率。
2. **经验法则**：一般建议线程数量与CPU核心数相匹配或稍多一些，以充分利用多核处理器的优势。例如，如果有一个8核CPU，可以尝试创建8到16个线程。
3. **动态调整**：在某些情况下，可以使用动态调整策略，根据当前的工作负载和系统资源实时调整线程数量。

总之，进程能够创建的线程数量是一个复杂的多因素问题，需要综合考虑系统资源、应用程序特性和性能需求。

### 1.6.0 fork进程
- 一个现有的进程可以通过fork函数来创建一个新的进程，这个进程通常称为子进程。fork函数原型如下：
```cpp
#include<unistd.h>
pid_t fork(void);
```
- 如果调用成功，它将返回两次，子进程返回值是0；父进程返回的是非0正值，表示子进程的进程id；如果调用失败将返回-1，并且置errno变量
- 一个进程可以有多个子进程，但是一个子进程同一时刻最多只有一个父进程。子进程可以通过getppid获取父进程的进程id，但是父进程却没法获取，因此需要在fork后就得到子进程的进程id。
### 1.7.0 fork做了什么？
- fork被调用后，子进程拥有父进程的副本，因此它拥有父进程的数据空间，堆栈等。
- 但是由于fork之后通常会调用exec函数去执行与原进程不想关的程序，因此fork时直接拷贝父进程的副本显得没有必要。为了提高fork的效率，采用了一种写时复制的技术。即fork之后，子进程名义上拥有父进程的副本，但是实际上和父进程共用，只有当父子进程中有一个试图修改这些区域时，才会以页为单位创建一个真正的副本。
### 1.7.1 fork和vfork的区别
在实现写时复制之前，Unix的设计者们就一直很关注在fork后立刻执行exec所造成的地址空间的浪费。BSD的开发者们在3.0的BSD系统中引入了vfork( )系统调用。
```cpp
#include <sys/types.h>
#include <unistd.h>
pid_t vfork(void);
```
除了子进程必须要立刻执行一次对exec的系统调用，或者调用_exit( )退出，对vfork( )的成功调用所产生的结果和fork( )是一样的。vfork( )会挂起父进程直到子进程终止或者运行了一个新的可执行文件的映像。通过这样的方式，vfork( )避免了地址空间的按页复制。在这个过程中，父进程和子进程共享相同的地址空间和页表项。实际上vfork( )只完成了一件事：复制内部的内核数据结构。因此，子进程也就不能修改地址空间中的任何内存。
vfork( )是一个历史遗留产物，Linux本不应该实现它。需要注意的是，即使增加了写时复制，vfork( )也要比fork( )快，因为它没有进行页表项的复制。然而，写时复制的出现减少了对于替换fork( )争论。实际上，直到2.2.0内核，vfork( )只是一个封装过的fork( )。因为对vfork( )的需求要小于fork( )，所以vfork( )的这种实现方式是可行的。
### 1.7.2 写时复制
Linux采用了写时复制的方法，以减少fork时对父进程空间进程整体复制带来的开销。

写时复制是一种采取了惰性优化方法来避免复制时的系统开销。它的前提很简单：如果有多个进程要读取它们自己的那部门资源的副本，那么复制是不必要的。每个进程只要保存一个指向这个资源的指针就可以了。只要没有进程要去修改自己的“副本”，就存在着这样的幻觉：每个进程好像独占那个资源。从而就避免了复制带来的负担。如果一个进程要修改自己的那份资源“副本”，那么就会复制那份资源，并把复制的那份提供给进程。不过其中的复制对进程来说是透明的。这个进程就可以修改复制后的资源了，同时其他的进程仍然共享那份没有修改过的资源。所以这就是名称的由来：在写入时进行复制。

写时复制的主要好处在于：如果进程从来就不需要修改资源，则不需要进行复制。惰性算法的好处就在于它们尽量推迟代价高昂的操作，直到必要的时刻才会去执行。

在使用虚拟内存的情况下，写时复制（Copy-On-Write）是以页为基础进行的。所以，只要进程不修改它全部的地址空间，那么就不必复制整个地址空间。在fork( )调用结束后，父进程和子进程都相信它们有一个自己的地址空间，但实际上它们共享父进程的原始页，接下来这些页又可以被其他的父进程或子进程共享。

写时复制在内核中的实现非常简单。与内核页相关的数据结构可以被标记为只读和写时复制。如果有进程试图修改一个页，就会产生一个缺页中断。内核处理缺页中断的方式就是对该页进行一次透明复制。这时会清除页面的COW属性，表示着它不再被共享。

现代的计算机系统结构中都在内存管理单元（MMU）提供了硬件级别的写时复制支持，所以实现是很容易的。

在调用fork( )时，写时复制是有很大优势的。因为大量的fork之后都会跟着执行exec，那么复制整个父进程地址空间中的内容到子进程的地址空间完全是在浪费时间：如果子进程立刻执行一个新的二进制可执行文件的映像，它先前的地址空间就会被交换出去。写时复制可以对这种情况进行优化。
### 1.7.3 fork和vfork的区别：
1. fork( )的子进程<font color='red'>拷贝</font>父进程的数据段和代码段；vfork( )的子进程与父进程<font color='red'>共享</font>数据段
2. fork( )的父子进程的<font color='red'>执行次序</font>不确定；vfork( )保证子进程先运行，在调用exec或exit之前与父进程数据是共享的，在它调用exec或exit之后父进程才可能被调度运行。
3. vfork( )保证子进程先运行，在它调用exec或exit之后父进程才可能被调度运行。如果在调用这两个函数之前子进程依赖于父进程的进一步动作，则会导致死锁。
4. 当需要改变共享数据段中变量的值，则拷贝父进程。
### 1.8. 子进程继承了父进程哪些属性？
由于子进程是父进程的一个副本，所以父进程有的属性，子进程也都有，这些属性包括
- 打开的文件描述符
- 会话ID
- 根目录
- 资源限制
- 工作目录
- 进程组ID
- 控制终端
- 环境
- …
### 1.9.父子进程有哪些不同？
- fork之后的返回值不同，进程ID也不同
- 子进程未处理信号设置为空
- 子进程不继承父进程设置的文件锁
- 一般子进程会执行与父进程不完全一样的代码流程

## 1.10 协程，
### 1.10.1 协程概念
又称微线程，纤程，英文名Coroutine。
协程（Coroutine，又称微线程）是一种比线程更加轻量级的存在，协程不是被操作系统内核所管理，而完全是由程序所控制。

协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行，协程之间的切换不需要涉及任何系统调用或任何阻塞调用。

### 1.10.2 协程和线程区别
#### 1.10.2.1 可中断，不涉及阻塞调用
协程可以比作子程序，但执行过程中，子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。协程之间的切换不需要涉及任何系统调用或任何阻塞调用
#### 1.10.2.2 效率高，不需要切换
那和多线程比，协程最大的优势就是协程极高的执行效率。协程相比线程节省线程创建和切换的开销。
线程的阻塞状态是由操作系统内核来完成，发生在内核态上，协程只在一个线程中执行，是子程序之间的切换，发生在用户态上。
因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。
#### 1.10.2.3 不存在同时写变量冲突
第二大优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。
### 1.10.3 协程 其他
在协程上利用多核CPU呢——多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。
Python对协程的支持还非常有限，用在generator中的yield可以一定程度上实现协程。虽然支持不完全，但已经可以发挥相当大的威力了。

### 1.10.4 协程适用于？
协程适用于IO阻塞且需要大量并发的场景，当发生IO阻塞，由协程的调度器进行调度，通过将数据流yield掉，并且记录当前栈上的数据，阻塞完后立刻再通过线程恢复栈，并把阻塞的结果放到这个线程上去运行。

# 二、进程间通信
### 2.1 进程间通信的方式
进程间通信主要包括
管道、
系统IPC（包括消息队列、信号量、信号、共享内存等）、
以及套接字socket。


### 2.1.1 普通管道PIPE：
> 管道主要包括无名管道和命名管道:
> 无名管道可用于具有亲缘关系的父子进程间的通信，
> 有名管道除了具有管道所具有的功能外，它还允许无亲缘关系进程间的通信
1. 它是**半双工**的（即数据只能在一个方向上流动），具有固定的读端和写端
2. 它只能用于具有亲缘关系的进程之间的通信（也是**父子进程**或者兄弟进程之间）
3. 它可以看成是一种特殊的文件，对于它的读写也可以使用普通的read、write等函数。但是它不是普通的文件，并不属于其他任何文件系统，并且只存在于内存中。
4. 使用popen函数和pclose函数结合来执行系统命令，就用到了管道
```cpp
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

int main() {
    // 创建无名管道
    int pipefd[2];
    if (pipe(pipefd) == -1) {
        printf("无法创建无名管道\n");
        return 1;
    }

    pid_t pid = fork();

    if (pid == -1) {
        printf("无法创建子进程\n");
        return 1;
    }
    else if (pid == 0) {
        // 子进程 - 写入数据到无名管道
        close(pipefd[0]); // 关闭读取端

        char *data = "Hello, Parent Process!";
        write(pipefd[1], data, strlen(data) + 1);

        close(pipefd[1]);
        exit(0);
    }
    else {
        // 父进程 - 从无名管道读取数据
        close(pipefd[1]); // 关闭写入端

        char data[100];
        read(pipefd[0], data, sizeof(data));

        printf("收到的数据：%s\n", data);

        close(pipefd[0]);
    }

    return 0;
}
```
### 2.1.2 命名管道FIFO：
1. FIFO可以在无关的进程之间交换数据
2. FIFO有路径名与之相关联，它以一种特殊设备文件形式存在于文件系统中。
3. `int mkfifo(const char *path, mode_t mode);`

示例- 进程1 - 写入数据到命名管道
```cpp
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <fcntl.h>
#include <sys/stat.h>
#include <sys/types.h>
#include <unistd.h>

int main() {
    // 创建命名管道
    mkfifo("myfifo", 0666);

    // 打开命名管道以进行写入
    int fd = open("myfifo", O_WRONLY);

    if (fd == -1) {
        printf("无法打开命名管道\n");
        return 1;
    }

    // 写入数据到命名管道
    char *data = "Hello, Process 2!";
    write(fd, data, strlen(data)+1);

    // 关闭命名管道
    close(fd);

    return 0;
}
```
进程2 - 读取数据从命名管道

```cpp
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <fcntl.h>
#include <sys/stat.h>
#include <sys/types.h>
#include <unistd.h>

int main() {
    // 打开命名管道以进行读取
    int fd = open("myfifo", O_RDONLY);

    if (fd == -1) {
        printf("无法打开命名管道\n");
        return 1;
    }

    // 读取数据从命名管道
    char data[100];
    read(fd, data, sizeof(data));

    // 显示读取到的数据
    printf("收到的数据：%s\n", data);

    // 关闭命名管道
    close(fd);

    // 删除命名管道
    unlink("myfifo");

    return 0;
}
```
在这个例子中，进程1创建了一个命名管道 "myfifo" 并向其中写入数据。进程2打开同一个命名管道并从中读取数据。
通过命名管道，这两个进程实现了简单的进程间通信。请注意，在使用命名管道进行通信之前，需要确保命名管道已被创建（可以使用 mkfifo 函数创建），并且两个进程都有权限访问该命名管道。
### 2.1.3 系统IPC 消息队列
1. 消息队列，是消息的链表，存放在内核中。一个消息队列由一个标识符（即队列ID）来标记。进程可以从中读写数据。
(消息队列克服了信号传递信息少，管道只能承载无格式字节流以及缓冲区大小受限等特点)
2. 具有写权限得进程可以按照一定得规则向消息队列中添加新信息；
3. 对消息队列有读权限得进程则可以从消息队列中读取信息；
4. 与管道和FIFO不同，进程可以在没有另外一个进程等待读的情况下进行写。但是对于消息队列，一个进程往消息队列中写入数据后退出，另外一个进程仍然可以打开并读取消息。

特点：
1. 消息队列是面向记录的，其中的消息具有特定的格式以及特定的优先级。
2. 消息队列独立于发送与接收进程。进程终止时，消息队列及其内容并不会被删除。
3. 消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取。

进程1 - 发送消息到消息队列

```cpp
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/types.h>
#include <sys/ipc.h>
#include <sys/msg.h>

struct msgbuf {
    long mtype;
    char mtext[100];
};

int main() {
    // 创建消息队列
    key_t key = ftok(".", 'q');
    int msgid = msgget(key, IPC_CREAT | 0666);
    if (msgid == -1) {
        printf("无法创建消息队列\n");
        return 1;
    }

    // 发送消息到消息队列
    struct msgbuf message;
    message.mtype = 1;  // 消息类型，可以自定义
    strcpy(message.mtext, "Hello, Process 2!");

    if (msgsnd(msgid, &message, sizeof(message.mtext), 0) == -1) {
        printf("无法发送消息到消息队列\n");
        return 1;
    }

    return 0;
}
```
进程2 - 从消息队列接收消息

```cpp
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/types.h>
#include <sys/ipc.h>
#include <sys/msg.h>

struct msgbuf {
    long mtype;
    char mtext[100];
};

int main() {
    // 获取消息队列的标识符
    key_t key = ftok(".", 'q');
    int msgid = msgget(key, 0666);
    if (msgid == -1) {
        printf("无法获取消息队列\n");
        return 1;
    }

    // 接收消息队列中的消息
    struct msgbuf message;
    if (msgrcv(msgid, &message, sizeof(message.mtext), 1, 0) == -1) {
        printf("无法接收消息队列中的消息\n");
        return 1;
    }

    printf("收到的消息：%s\n", message.mtext);

    // 删除消息队列
    if (msgctl(msgid, IPC_RMID, NULL) == -1) {
        printf("无法删除消息队列\n");
        return 1;
    }

    return 0;
}
```
在这个示例中，进程1通过调用 msgget() 来创建或获取一个已经存在的消息队列。然后，它使用 msgsnd() 向消息队列发送一条消息，消息类型为1。进程2使用 msgget() 获取同一个消息队列，并使用 msgrcv() 从队列中接收类型为1的消息。最后，进程2通过 msgctl() 删除消息队列。

请注意，消息队列是一种进程间通信的方式，可以进行多对多的通信。不同于无名管道，消息队列可以在没有父子关系的进程之间进行通信，并且消息可以按照类型进行区分。

### 2.2.4 系统IPC 信号量semaphore
信号量（semaphore）与已经介绍过的 IPC 结构不同，它是一个计数器，可以用来控制多个进程对共享资源的访问。信号量用于实现进程间的互斥与同步，而不是用于存储进程间通信数据。
特点：

1. 提供了一个不同进程或者进程的不同线程之间访问同步的手段:
2. 信号量用于进程间同步，若要在进程间传递数据需要结合共享内存。检查控制该资源的信号量
3. 信号量基于操作系统的 PV 操作，程序对信号量的操作都是原子操作。
4. 每次对信号量的 PV 操作不仅限于对信号量值加 1 或减 1，而且可以加减任意正整数。如果信号量值大于0，则资源可用，并且将其减1，表示当前已被使用
5. 支持信号量组。
6. 如果信号量值为0，则进程休眠直至信号量值大于0
### 2.2.5 系统IPC 信号signal
信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。

### 2.2.6 系统IPC 共享内存（Shared Memory）
它使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据得更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等
特点：

1. 共享内存是最快的一种IPC，因为进程是直接对内存进行存取
2. 因为多个进程可以同时操作，所以需要进行同步
3. 信号量+共享内存通常结合在一起使用，信号量用来同步对共享内存的访问
4. 允许多个进程共享一个给定的存储区，
### 2.2.7 UNXI域套接字
- 不需要执行协议处理，例如计算校验和，发送确认报文等等，它仅仅复制数据。
- 只适用于同一台计算机上的进程间通信
### 2.2.8 套接字SOCKET：
socket也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同主机之间的进程通信。

### 2.2.9 线程间通信的方式:
* 临界区：
  - 通过多线程的串行化来访问公共资源或一段代码，速度快，适合控制数据访问；
* 互斥量Synchronized/Lock：
  - 采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问
* 信号量Semphare：
  - 为控制具有有限数量的用户资源而设计的，它允许多个线程在同一时刻去访问同一个资源，但一般需要限制同一时刻访问此资源的最大线程数目。
* 事件(信号)，Wait/Notify：
  - 通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作;线程可以设置或等待事件。当事件被设置时，等待该事件的一个或多个线程将被唤醒。

原子操作：使用原子操作可以在不使用锁的情况下对共享数据进行操作，这样可以避免竞态条件。
屏障（Barriers）：允许多个线程在某个点上同步。当所有线程都到达屏障时，它们将同时继续执行。

条件变量（Condition Variable）：条件变量是一种线程同步的机制，用于线程间的等待和通知。线程可以通过条件变量等待某个条件的满足，当条件满足时，其他线程可以通过条件变量发出通知，唤醒等待的线程。
共享内存：线程可以通过读写共享内存中的变量或数据结构来通信。为了防止数据竞争和不一致，通常需要使用互斥量（mutexes）或其他同步原语进行保护。
管道（Pipes）：虽然通常用于进程间通信，但它们也可以用于线程间通信。一个线程向管道写数据，而另一个线程从管道读数据。
消息队列：线程将消息发送到队列，其他线程可以从队列中读取消息。这是一种避免共享数据并减少同步问题的方法。

### 2.2.10 barrier
#### 2.2.10.1 atomic原子操作计数器
在C++中，内存屏障（memory barrier）或内存栅栏（memory fence）是一种同步机制，它确保了某些内存操作的顺序。这对于多线程编程非常重要，因为它可以帮助防止由于编译器优化和处理器乱序执行导致的数据竞争问题。

C++11引入了`std::atomic`库来提供原子操作，并且通过这些原子类型提供了内置的内存顺序保证。`std::atomic`允许你指定不同的内存顺序，从而控制内存屏障的行为。以下是一些常用的内存顺序：

- `std::memory_order_relaxed`：最弱的内存顺序，只保证原子操作本身的原子性，不提供任何顺序上的保证。
- `std::memory_order_acquire`：获取操作，确保所有后续读取或写入都发生在该操作之后。
- `std::memory_order_release`：释放操作，确保所有先前的读取或写入都发生在该操作之前。
- `std::memory_order_acq_rel`：获取-释放操作，结合了`acquire`和`release`的特点。
- `std::memory_order_seq_cst`：序列一致性，最强的内存顺序，确保所有线程看到的操作顺序是一致的。

下面是一个简单的例子，展示了如何使用`std::atomic`和内存顺序来实现一个线程安全的计数器：

```cpp
#include <iostream>
#include <thread>
#include <atomic>

std::atomic<int> counter(0);

void increment() {
    for (int i = 0; i < 100000; ++i) {
        // 使用memory_order_relaxed，因为这里我们只关心计数器的原子性
        counter.fetch_add(1, std::memory_order_relaxed);
    }
}

void print_counter() {
    // 在打印之前使用memory_order_acquire，确保能看到所有之前的更新
    int value = counter.load(std::memory_order_acquire);
    std::cout << "Counter: " << value << std::endl;
}

int main() {
    std::thread t1(increment);
    std::thread t2(increment);

    t1.join();
    t2.join();

    // 在主线程中使用memory_order_acquire，确保能看到所有线程的更新
    print_counter();

    return 0;
}
```

在这个例子中，`counter`是一个原子整数，用于在多个线程之间共享。`increment`函数在一个循环中递增计数器，而`print_counter`函数则打印计数器的值。

- `fetch_add(1, std::memory_order_relaxed)`：这个调用使用了`relaxed`内存顺序，因为我们只需要保证单个操作的原子性，而不关心其他操作的顺序。
- `load(std::memory_order_acquire)`：在打印计数器值之前，我们使用`acquire`内存顺序来确保能够看到所有之前的更新。

如果你需要更严格的顺序保证，可以将`fetch_add`中的`memory_order_relaxed`替换为`memory_order_release`或其他适当的内存顺序。

请注意，内存屏障的正确使用是非常微妙的，不当的使用可能会导致难以调试的并发错误。因此，在设计多线程程序时，务必仔细考虑你的需求并选择合适的内存顺序。

在C++中实现多线程的内存栅格（也称为栅栏或屏障）通常是为了确保一组线程在继续执行之前都到达某个点。这可以用于协调多个线程，使得它们能够同步地进入下一个阶段的工作。

#### 2.2.10.2 atomic+condition实现栅栏计数器
C++11 标准引入了 `<thread>` 和 `<mutex>` 库，提供了基本的线程和互斥锁功能。然而，标准库并没有直接提供栅栏的功能。不过，我们可以使用 `std::condition_variable` 和 `std::mutex` 来手动实现一个简单的栅栏。

下面是一个简单的栅栏实现示例：

```cpp
#include <iostream>
#include <thread>
#include <vector>
#include <condition_variable>
#include <mutex>

class Barrier {
public:
    // 构造函数接受需要等待的线程数
    explicit Barrier(int count) : count_(count), at_barrier_(0) {}

    void wait() {
        std::unique_lock<std::mutex> lock(mutex_);
        ++at_barrier_;  // 计算到达栅栏的线程数量
        if (at_barrier_ == count_) {  // 如果所有线程都到了
            at_barrier_ = 0;  // 重置计数器
            cond_.notify_all();  // 唤醒所有等待的线程
        } else {
            cond_.wait(lock);  // 等待被唤醒
        }
    }

private:
    int count_;  // 总共需要等待的线程数
    int at_barrier_;  // 当前已经到达栅栏的线程数
    std::mutex mutex_;
    std::condition_variable cond_;
};

// 工作线程的函数
void threadFunc(Barrier& barrier, int id) {
    std::cout << "Thread " << id << " before the barrier.\n";
    barrier.wait();
    std::cout << "Thread " << id << " after the barrier.\n";
}

int main() {
    const int num_threads = 5;
    Barrier barrier(num_threads);
    std::vector<std::thread> threads;

    for (int i = 0; i < num_threads; ++i) {
        threads.emplace_back(threadFunc, std::ref(barrier), i);
    }

    for (auto& t : threads) {
        t.join();
    }

    return 0;
}
```

在这个例子中，`Barrier` 类维护了一个计数器 `at_barrier_` 来跟踪有多少个线程已经到达栅栏。当所有的线程都到达时，它会重置计数器并通知所有等待的线程。每个线程在调用 `barrier.wait()` 时都会检查是否所有线程都已经到达，如果没有，则该线程会被阻塞直到被唤醒。

这个实现是基本的，并且适用于大多数情况。但是，对于更复杂的需求，可能需要考虑其他因素，比如错误处理、超时支持等。此外，如果你正在使用的是 C++20 或更高版本，可以考虑使用 `std::latch` 或 `std::barrier`，它们提供了更加方便的API来处理这种情况。
#### 2.2.10.3 C++20的latch和barrier
在C++20中，标准库引入了两个新的同步原语：`std::latch` 和 `std::barrier`。这两个工具可以用来简化多线程编程中的同步问题。

##### 2.2.10.3.1 `std::latch`

`std::latch` 是一个一次性的栅栏，它允许一组线程等待直到所有线程都到达某个点。一旦所有线程到达，栅栏就会被释放，并且不能再次使用。

```cpp
#include <iostream>
#include <thread>
#include <latch>

void worker(std::latch& l, int id) {
    std::cout << "Worker " << id << " before the latch.\n";
    l.arrive_and_wait();  // 等待直到所有的线程都到达
    std::cout << "Worker " << id << " after the latch.\n";
}

int main() {
    const int num_workers = 5;
    std::latch l(num_workers);  // 初始化latch，设置计数为num_workers

    std::vector<std::thread> workers;
    for (int i = 0; i < num_workers; ++i) {
        workers.emplace_back(worker, std::ref(l), i);
    }

    for (auto& t : workers) {
        t.join();
    }

    return 0;
}
```

在这个例子中，`std::latch` 被初始化为等待5个线程。每个线程调用 `arrive_and_wait()` 来减少计数器并等待，直到计数器达到0。当所有线程都调用了 `arrive_and_wait()` 后，所有线程都会继续执行。

##### 2.2.10.3.2 `std::barrier`

`std::barrier` 与 `std::latch` 类似，但它允许多次使用。它会在所有线程到达后重置，以便下一轮的同步。

```cpp
#include <iostream>
#include <thread>
#include <barrier>

void worker(std::barrier& b, int id) {
    for (int round = 1; round <= 3; ++round) {
        std::cout << "Worker " << id << " in round " << round << " before the barrier.\n";
        b.arrive_and_wait();  // 等待直到所有的线程都到达
        std::cout << "Worker " << id << " in round " << round << " after the barrier.\n";
    }
}

int main() {
    const int num_workers = 5;
    std::barrier b(num_workers, []{  // 可选的回调函数
        std::cout << "Barrier reached by all threads, ready for next round.\n";
    });

    std::vector<std::thread> workers;
    for (int i = 0; i < num_workers; ++i) {
        workers.emplace_back(worker, std::ref(b), i);
    }

    for (auto& t : workers) {
        t.join();
    }

    return 0;
}
```

在这个例子中，`std::barrier` 被初始化为等待5个线程，并提供了一个可选的回调函数，该函数在每次所有线程都到达屏障时执行。每个线程会循环三次，每轮中调用 `arrive_and_wait()`。当所有线程都到达屏障后，它们会继续执行下一轮，而屏障则自动重置以供下一轮使用。

这些新特性使得编写多线程程序变得更加简洁和直观。如果你正在使用C++20或更高版本，推荐使用这些内置的同步原语来处理多线程同步问题。
## 2.3 为什么用进程间通信？
### 2.3.1 进程间通信 背景
- 进程是一个独立的资源分配单元，不同进程（这里所说的进程通常指的是用户进程）之间的资源是独立的，没有关联，不能在一个进程中直接访问另一个进程的资源（例如打开的文件描述符）。
- 但是，进程不是孤立的，不同的进程需要进行信息的交互和状态的传递等，因此需要进程间通信( IPC：Inter Processes Communication )。
### 2.3.2 进程间通信 目的
#### 2.3.2.1 <font color='red'>数据传输：</font>
一个进程需要将它的数据发送给另一个进程。
#### 2.3.2.2 <font color='red'>通知事件：</font>
一个进程需要向另一个或一组进程发送消息，通知它（它们）发生了某种事件（如进程终止时要通知父进程）。
#### 2.3.2.3 <font color='red'>资源共享：</font>
多个进程之间共享同样的资源。为了做到这一点，需要内核提供互斥和同步机制。
#### 2.3.2.4 <font color='red'>进程控制：</font>
有些进程希望完全控制另一个进程的执行（如 Debug 进程），此时控制进程希望能够拦截另一个进程的所有陷入和异常，并能够及时知道它的状态改变。
### 2.4 为什么用共享内存
- 为了在多个进程间交换信息，内核专门留出了一块内存区，可以由需要访问的进程将其映射到自己的私有地址空间。进程就可以直接读写这一块内存而不需要进行数据的拷贝，从而大大提高效率。
- 共享内存区时可用IPC形式中最快的。内存区共享它的进程的地址空间，进程间的数据传递不再涉及内核。绕过内核。
# 三、操作系统
### 3.0 操作系统
## 3.1 操作系统中的程序的内存结构
## 3.2 缺页中断
## 3.3 修改文件最大句柄数？
1. ulimit -n <可以同时打开的文件数>
2. vi /etc/security/limits.conf

在 Linux 系统中，句柄（或称为文件描述符）的数量是有限制的。这些限制分为两种：系统级限制和用户级限制。

### 3.3.1 系统级限制 `/proc/sys/fs/file-max`
系统级限制指的是整个系统能够打开的最大文件描述符数量。这个限制可以通过 `/proc/sys/fs/file-max` 文件来查看和修改。

#### 3.3.1.1 查看系统级限制
```sh
cat /proc/sys/fs/file-max
```

#### 3.3.1.2 修改系统级限制
要修改这个值，你需要有超级用户权限（root）。你可以临时修改它，也可以永久修改它。

- **临时修改**：
  ```sh
  echo 100000 > /proc/sys/fs/file-max
  ```

- **永久修改**：
  编辑 `/etc/sysctl.conf` 文件，添加或修改以下行：
  ```sh
  fs.file-max = 100000
  ```
  然后运行 `sysctl -p` 使更改生效。

### 3.3.2 用户级限制`ulimit`or`/etc/security/limits.conf`
用户级限制指的是每个进程可以打开的最大文件描述符数量。这个限制可以通过 `ulimit` 命令或 `/etc/security/limits.conf` 文件来查看和修改。

#### 3.3.2.1 查看用户级限制
```sh
ulimit -n
```

#### 3.3.2.2 修改用户级限制
- **临时修改**：
  你可以在当前 shell 会话中使用 `ulimit` 命令来临时修改限制：
  ```sh
  ulimit -n 4096
  ```

- **永久修改**：
  要永久修改用户级限制，你需要编辑 `/etc/security/limits.conf` 文件。例如，要为所有用户设置软限制和硬限制为 4096：
  ```sh
  * soft nofile 4096
  * hard nofile 4096
  ```
  如果你只想为特定用户设置限制，可以将 `*` 替换为用户名。例如，为用户 `john` 设置限制：
  ```sh
  john soft nofile 4096
  john hard nofile 4096
  ```

  修改后，用户需要重新登录才能使新的限制生效。

### 3.3.3 注意事项
- **软限制 (soft limit)** 是用户可以自行修改的限制。
- **硬限制 (hard limit)** 是管理员设置的上限，用户不能超过这个限制。

### 3.3.4 验证修改
你可以通过编写一个简单的程序或脚本来验证新的限制是否生效。例如，下面是一个简单的 C 程序来打开多个文件描述符并检查限制：

```c
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <unistd.h>

int main() {
    int fd, i;
    for (i = 0; ; i++) {
        char filename[20];
        sprintf(filename, "/tmp/file%d", i);
        fd = open(filename, O_CREAT | O_RDWR, 0644);
        if (fd == -1) {
            perror("open");
            printf("Max file descriptors: %d\n", i);
            return 0;
        }
    }
    return 0;
}
```

编译并运行这个程序，它会不断打开文件直到达到文件描述符的限制，并输出最大打开的文件描述符数量。

通过以上方法，你可以查看和修改 Linux 系统中的文件描述符限制。根据你的需求，选择合适的方法进行调整。
### 3.4 并发(concurrency)和并行(parallelism)
并发（concurrency）：
- 指宏观上看起来两个程序在同时运行，比如说在单核cpu上的多任务。但是从微观上看两个程序的指令是交织着运行的，你的指令之间穿插着我的指令，我的指令之间穿插着你的，在单个周期内只运行了一个指令。这种并发并不能提高计算机的性能，只能提高效率。

并行（parallelism）：
- 指严格物理意义上的同时运行，比如多核cpu，两个程序分别运行在两个核上，两者之间互不影响，单个周期内每个程序都运行了自己的指令，也就是运行了两条指令。这样说来并行的确提高了计算机的效率。所以现在的cpu都是往多核方面发展

## 3.5 死锁
### 3.5.1 死锁原因
在死锁时，线程/进程间相互等待资源，而又不释放自身的资源，导致无穷无尽的等待，其结果是任务永远无法执行完成。
如果一组进程中每一个进程都在等待仅由该组进程中的其他进程才能引发的事件，那么该组进程是死锁的。

### 3.5.2 死锁必要条件
#### 1.互斥：
某资源只能被一个进程使用，其他进程请求时，只能等待，直到资源使用完毕后释放
#### 2.请求和保持条件，占有且申请：
至少保持了一个资源，又提出新要求，而这个资源被其他进程占用，自己占用却不释放
#### 3.不剥夺条件，不可抢占：
任何一个资源在没被该进程释放之前，任何其他进程都无法对他剥夺占用
#### 4.循环等待条件：
当发生死锁时，所等待的进程必定会形成一个环路（类似于死循环），造成永久阻塞
### 3.5.3 死锁避免
1.尽量避免同时只对一个互斥锁上锁
2.互斥锁保护区域不要使用操作其他互斥锁的代码，因为用户的代码可能操作了其他的互斥锁
3.如果想同时对多个互斥锁上锁，要使用std::lock(my_mutex_1, my_mutex_2);一旦有一个互斥量不能被锁，线程就会卡在这里，直至两个锁可以被同时锁成功，
4.给锁定义顺序（使用层次锁，或者比较地址等），每次以同样的顺序进行上锁

### 3.5.4 死锁处理
破坏除了互斥以外的死锁必要条件（打破互斥即使可以同时访问，无价值。）
#### 3.5.4.1 <font color='red'>打破不可抢占</font>，
允许<font color='red'>强行从占有者那里夺取资源</font>。（一个进程已占有但不能满足时要释放资源重新申请。）降低系统性能；
#### 3.5.4.2 打破占有且申请，
资源预先分配，<font color='red'>一次性申请全部资源</font>，不满足则不申请不运行，
缺点：
1).许多情况下并不知道它需要的全部资源，动态的，不可预测的
2).资源利用率低，资源一直被长期占用，浪费资源
3).降低并发性，资源有限+浪费，能分配到全部资源的进程少

#### 3.5.4.3 打破循环等待条件
实行<font color='red'>资源有序分配策略</font>。采用这种策略，即把资源事先分类编号，按号分配，使进程在申请，占用资源时不会形成环路。所有进程对资源的请求必须严格按资源序号递增的顺序提出。进程占用了小号资源，才能申请大号资源，就不会产生环路，从而预防了死锁。
缺点：
1).限制进程对资源的请求，同时合理编号比较难，增加系统开销
2).为遵循编号次序，暂时不使用也需要提前申请，从而增加了进程对资源的占用时间。
#### 避免
在资源动态分配过程中，用某种方式防止系统进入不安全的状态
#### 检测
运行时出现死锁，能及时发现死锁，
#### 解除
解脱进程，通常撤销进程，回收资源，再分配给正处于阻塞状态的进程。

？如果已经发生了死锁该怎么办？
### 3.6 为什么用线程池？
#### 3.6.1 创建资源
1. 线程创建所需时间为T1，线程执行任务时间为T2，线程销毁时间为T3，而往往T1+T3>T2。所以频繁创建线程会损坏而外的时间。
2. 如果有任务来了，再去创建线程的话效率比较低。
3. 线程池可以管理控制线程，线程是稀缺资源，如果无休止的创建会消耗系统资源，还会降低系统稳定性。使用线程池可以进行统一分配，方便调优和监控。
4. 线程池提供队列，存放缓冲等待执行任务。
### 3.6.2 线程池通常适合下面的几个场合：
1. 单位时间内处理的任务数较多，且每个任务的执行时间较短
2. 对实时性要求较高的任务，如果接受到任务后在创建线程，再执行任务，可能满足不了实时要求，因此必须采用线程池进行预创建。

## 3.7 生产者消费者模型
### 3.7.1 生产者消费者模型 解耦
- 需要有一个交易场所。（存储数据的地方，可能是一个队列、栈或者其他数据结构）
- 生产者：负责产生数据，然后把数据放到交易场所中。
- 消费者：负责消费数据，从交易场所中获取走。
- 通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。这个阻塞队列就是用来给生产者和消费者解耦的。
### 3.7.2 生产者消费者模型 优点
  * 解耦
  * 并发
  * 忙闲不均
在生产者消费者模型中，生产者和消费者之间通过共享的缓冲区进行通信。当生产者生成数据时，将数据放入缓冲区；当消费者需要消费数据时，从缓冲区中取出数据。通过合理地设计缓冲区的大小，可以使生产者和消费者之间的速度不匹配问题得到缓解。
当生产者生成数据的速度快于消费者消费数据的速度时，生产者可以将数据放入缓冲区并继续生成下一个数据，而不需要等待消费者消费。这样，生产者可以保持较高的工作效率，不会因为消费者速度慢而空闲等待。
同时，消费者可以按照自己的处理能力从缓冲区中取出数据进行消费，而不会受到生产者速度快的影响。消费者可以根据自己的处理能力合理安排消费数据的速度，不会因为生产者速度快而导致数据堆积或丢失。
因此，生产者消费者模型的忙闲不均特性使得生产者和消费者能够在各自的最大工作效率下进行工作，提高了多线程程序的整体性能。

### 3.7.3 生产者消费者模型 三种关系：
  * 消费者和消费者之间是互斥关系。两个消费者只有一个能拥有数据。
  * 生产者和生产者之间是互斥关系。两个生产者只有一个可以往里面存数据，不能同时存。
  * 生产者和消费者之间是同步互斥的关系。生产者和消费者必须按照一定的顺序执行。

### 3.8 Linux的4种锁机制：
mutex rwlock spinlock RCU
### 3.8.0 锁被释放时，唤醒的方式？
当多个线程试图获取同一互斥锁（mutex）时，只有一个线程能成功获取，其他的线程将被阻塞（即进入睡眠状态），等待该锁被释放。
当互斥锁被释放时，阻塞的线程通常会被唤醒来竞争该锁。但具体哪个线程能成功获取这个锁，或者说是否所有等待的线程都会被唤醒，这取决于操作系统的调度策略。
一些系统可能会选择只唤醒一个线程，让它获取锁。这被称为"唤醒一个"（waking one）策略。其他系统可能会选择唤醒所有等待的线程，并让它们竞争获取锁，这被称为"唤醒所有"（waking all）或 "thundering herd"策略。
"唤醒一个"策略可能更有效率，因为它避免了不必要的竞争。但"唤醒所有"策略可能在某些情况下更公平，尽管可能会导致资源竞争更激烈。
无论哪种策略，应用程序通常不需要（也不应该）关心这些细节。重要的是正确地使用互斥锁来保护对共享数据的访问，避免数据竞争和死锁。

### 3.8.1 互斥锁：mutex，
用于保证在**任何时刻，都只能有一个线程**访问该对象。
当获取锁操作失败时，线程会**进入睡眠，等待锁释放时被唤醒**

### 3.8.2 读写锁：rwlock，分为读锁和写锁。
处于读操作时，可以允许**多个线程同时获得读**操作。
但是同一时刻**只能有一个线程可以获得写锁**。其它获取写锁失败的线程都会**进入睡眠状态，直到写锁释放时被唤醒**。
注意：写锁会阻塞其它读写锁。当有一个线程获得写锁在写时，读锁也不能被其它线程获取；写者优先于读者（一旦有写者，则后续读者必须等待，唤醒时优先考虑写者）。
适用于读取数据的频率远远大于写数据的频率的场合。
### 3.8.3 自旋锁：spinlock，
在任何时刻同样**只能有一个线程访问对象**。
但是当获取锁操作失败时，**不会进入睡眠，而是会在原地自旋**，直到锁被释放。
这样节省了线程从睡眠状态到被唤醒期间的消耗，在加锁时间短暂的环境下会极大的提高效率。但如果加锁时间过长，则会非常浪费CPU资源。

### 3.8.4 RCU：即read-copy-update，
在修改数据时，首先需要读取数据，然后**生成一个副本，对副本进行修改**。
修改完成后，再将老数据update成新的数据。
使用RCU时，读者几乎不需要同步开销，既不需要获得锁，也不使用原子指令，不会导致锁竞争，因此就不用考虑死锁问题了。
而对于写者的同步开销较大，它需要复制被修改的数据，还必须使用锁机制同步并行其它写者的修改操作。在有大量读操作，少量写操作的情况下效率非常高。
- **用途**：专为大量读操作和少量写操作设计，尤其在读操作频繁且不需要立即反映最新写入状态的场景中。
- **特点**：RCU是一种非阻塞的锁机制，它允许读操作不受写操作的影响。当有写操作时，RCU会暂停写操作，直到所有活跃的读操作完成，然后在所谓的“静默期”（grace period）后执行写操作。这种方法避免了读操作被阻塞，提高了并发性能。
RCU（Read-Copy-Update）确实**不是传统意义上的锁，而是一种用于并发控制的机制**，特别是在处理读多写少的数据结构时。RCU允许读取操作在不阻塞的情况下进行，而更新操作则会在所有正在进行的读取操作完成后进行，这样就避免了在读取过程中加锁所带来的开销。

在RCU机制中，当要更新数据时，不会立即修改原始数据，而是创建一个数据的副本（Copy），然后在这个副本上进行更新（Update）。在所有的读取操作完成之后，也就是在所谓的“静默期”（grace period）结束后，旧的数据会被替换为更新后的数据副本。这个过程确保了读取操作的原子性，同时也避免了写操作导致的读取阻塞。

RCU在Linux内核中的实现

在Linux内核中，RCU的实现主要依赖于以下函数：
- `rcu_read_lock()` 和 `rcu_read_unlock()`：分别用于进入和退出RCU读取区域。
- `call_rcu()` 或 `schedule_rcu()`：用于调度RCU回调函数，在所有读取者离开RCU读取区域后执行。

示例代码 :下面是一个简化的示例，展示了如何在Linux内核中使用RCU机制：

```c
#include <linux/rcupdate.h>
#include <linux/slab.h>

struct my_data {
    int data;
    struct rcu_head rcu;
};

static DEFINE_RCU_POINTER(my_data_ptr, struct my_data *);

static void update_my_data(struct my_data *old_data)
{
    struct my_data *new_data;

    new_data = kmalloc(sizeof(*new_data), GFP_ATOMIC);
    if (new_data) {
        new_data->data = old_data->data + 1;
        call_rcu(&old_data->rcu, free_old_data);
        my_data_ptr = rcu_update_pointer(my_data_ptr, new_data);
    }
}

static void free_old_data(struct rcu_head *head)
{
    struct my_data *old_data;

    old_data = container_of(head, struct my_data, rcu);
    kfree(old_data);
}

void read_my_data(void)
{
    struct my_data *data;

    rcu_read_lock();
    data = rcu_dereference(my_data_ptr);
    /* 读取并使用 data */
    rcu_read_unlock();
}
```

在上面的代码中，`update_my_data` 函数用于更新数据，它首先分配一个新的数据结构，并在其中设置新值，然后调用 `call_rcu` 来安排一个回调函数 `free_old_data`，该回调函数会在所有读取者退出 RCu 读取区域后执行，释放旧的数据结构。`read_my_data` 函数则演示了如何在读取数据时使用 RCU 机制。

请注意，实际的内核代码可能包含更多的错误检查和复杂性，上述代码仅用于说明目的。在编写内核模块时，应当遵循内核的编码标准和最佳实践。

### 3.8.5 **RCU（Read-Copy-Update）机制下的数据一致性分析**

在并发编程中，RCU（Read-Copy-Update）是一种高效的无锁同步机制，**特别适用于读多写少的场景**。其核心思想是通过延迟旧数据的回收来避免读写冲突，但写入者之间的竞争需要额外处理。以下是关于用户问题的详细解答：

---

#### **3.8.5.1 RCU 的基本原理**
- **读操作**：读者直接访问数据，无需加锁（通过内存屏障或原子操作保证可见性）。
- **写操作**：
  1. **生成副本**：写入者创建数据的副本。
  2. **修改副本**：在副本上进行修改。
  3. **替换指针**：通过原子操作将指针指向新副本。
  4. **延迟回收**：等待所有读者退出临界区后，安全回收旧数据。

#### **3.8.5.2 并发写入时的潜在问题**
如果 **多个写入者同时修改同一数据**，且未使用额外同步机制，可能导致以下问题：
- **写写竞争（Write-Write Race）**：
  - 写入者A和B同时生成副本并修改。
  - 若两者都尝试替换指针，后提交的写入者会覆盖前者的修改，导致数据丢失。
- **读写不一致（Read-Write Inconsistency）**：
  - 写入者A替换指针后，写入者B仍在修改旧副本（未同步的情况下），此时读者可能访问到中间状态的旧数据。

#### **3.8.5.3 RCU 如何解决写入冲突？**
RCU **本身不处理写入者之间的竞争**，需结合其他同步机制（如互斥锁）来保证写入串行化：
- **单写入者场景**：天然安全，无需额外同步。
- **多写入者场景**：
  ```c
  // 示例：使用互斥锁保护 RCU 写入者
  static DEFINE_MUTEX(rcu_write_lock);

  void update_data() {
      mutex_lock(&rcu_write_lock);   // 写入者间互斥
      // 1. 生成副本
      struct data *new_copy = copy_data(old_ptr);
      // 2. 修改副本
      new_copy->value = new_value;
      // 3. 替换指针（原子操作）
      rcu_assign_pointer(global_ptr, new_copy);
      // 4. 同步等待旧数据回收（如 synchronize_rcu()）
      synchronize_rcu();
      kfree(old_ptr);
      mutex_unlock(&rcu_write_lock);
  }
  ```
  - **互斥锁的作用**：确保同一时间只有一个写入者操作数据，避免写写竞争。
  - **RCU 的作用**：保证读者无锁访问，同时安全回收旧数据。

#### **3.8.5.4 两个RCU是否会导致数据不一致？**
**问：**  
“如果使用 RCU 生成副本修改时，另一个锁（如互斥锁）生成的副本是否会导致数据不一致？”

**答：**  
- **若写入者未正确同步**：是的，可能引发不一致。例如：
  - 写入者A（使用 RCU）和写入者B（使用互斥锁）同时修改同一数据。
  - 若两者未协调，写入者B可能基于旧副本修改数据，覆盖写入者A的更新。
- **正确做法**：
  - **统一同步机制**：所有写入者必须使用同一锁（如互斥锁）来保证互斥。
  - **原子替换顺序**：确保指针替换和旧数据回收的原子性（由 RCU 机制保证）。

#### **3.8.5.5 实际场景模拟**
假设全局数据 `global_data` 被 RCU 保护，但写入者混合使用不同锁：
```c
// 错误示例：写入者A用 RCU，写入者B用独立锁
// 写入者A（RCU）：
rcu_read_lock();
struct data *new = copy_data(global_data);
new->value = 100;
rcu_assign_pointer(global_data, new);
synchronize_rcu();
kfree(old);
rcu_read_unlock();

// 写入者B（独立锁）：
mutex_lock(&other_lock);
struct data *new = copy_data(global_data); // 可能基于旧副本
new->value = 200;
global_data = new; // 直接覆盖，无 RCU 同步
mutex_unlock(&other_lock);
```
**结果**：  
写入者B的修改会覆盖写入者A的更新，且旧数据可能被提前回收，导致读者访问到无效内存。

#### **3.8.5.6 解决方案**
- **统一写入者锁**：所有写入者必须通过同一互斥锁进行同步。
  ```c
  static DEFINE_MUTEX(rcu_write_lock); // 所有写入者共享的锁

  void writer_a() {
      mutex_lock(&rcu_write_lock);
      // RCU 更新逻辑
      mutex_unlock(&rcu_write_lock);
  }

  void writer_b() {
      mutex_lock(&rcu_write_lock);
      // 其他更新逻辑（需与 RCU 兼容）
      mutex_unlock(&rcu_write_lock);
  }
  ```
- **避免混合同步机制**：不要将 RCU 与其他非协调的锁（如自旋锁、读写锁）混用。
- **明确数据生命周期**：确保旧数据在回收前未被其他写入者误用。

#### **3.8.5.7 性能与一致性的权衡**
| **场景**               | **同步机制**      | **一致性** | **性能**       |
|-------------------------|-------------------|------------|----------------|
| 纯读                   | RCU              | 最终一致   | 最优（无锁）   |
| 单写入者               | RCU + 互斥锁     | 强一致     | 高             |
| 多写入者（低竞争）     | RCU + 互斥锁     | 强一致     | 中等           |
| 多写入者（高竞争）     | 纯互斥锁         | 强一致     | 较低（锁竞争） |

#### **3.8.5.8 RCU 总结**
- **RCU 的优势**：无锁读取，适合读多写少。
- **写入者同步的必要性**：多写入者必须通过锁保证互斥。
- **混合机制的陷阱**：若未统一同步，会导致数据不一致或内存安全问题。

**正确实践**：  
- 对同一数据的写入者统一使用互斥锁。
- 严格遵循 RCU 的“生成-修改-替换-回收”流程。
- 避免将 RCU 与其他非协调的同步机制混合使用。

通过合理设计，RCU 可以在保证高性能的同时，实现数据一致性。
### 3.9 互斥锁和读写锁区别：

互斥锁：当获取锁操作失败时，线程会进入睡眠，等待锁释放时被唤醒。

读写锁：rwlock，可以允许多个线程同时获得读操作。但是同一时刻只能有一个线程可以获得写锁。其它获取写锁失败的线程都会进入睡眠状态，直到写锁释放时被唤醒。

1）读写锁区分读者和写者，而互斥锁不区分
2）互斥锁同一时间只允许一个线程访问该对象，无论读写；读写锁同一时间内只允许一个写者，但是允许多个读者同时读对象。

### 3.10 用户态和内核态区别
用户态和内核态是操作系统的两种运行级别，两者最大的区别就是特权级不同。
用户态拥有最低的特权级，内核态拥有较高的特权级。
运行在用户态的程序不能直接访问操作系统内核数据结构和程序。内核态和用户态之间的转换方式主要包括：系统调用，异常和中断。

## 3.11 系统调用是什么，你用过哪些系统调用
### 3.11.1 系统调用 概念：
在计算机中，系统调用（英语：system call），又称为系统呼叫，指运行在使用者空间的程序向操作系统内核请求需要更高权限运行的服务。系统调用提供了用户程序与操作系统之间的接口（即系统调用是用户程序和内核交互的接口）。
操作系统中的状态分为管态（核心态）和目态（用户态）。大多数系统交互式操作需求在内核态执行。如设备IO操作或者进程间通信。特权指令：一类只能在核心态下运行而不能在用户态下运行的特殊指令。不同的操作系统特权指令会有所差异，但是一般来说主要是和硬件相关的一些指令。用户程序只在用户态下运行，有时需要访问系统核心功能，这时通过系统调用接口使用系统调用。
应用程序有时会需要一些危险的、权限很高的指令，如果把这些权限放心地交给用户程序是很危险的(比如一个进程可能修改另一个进程的内存区，导致其不能运行)，但是又不能完全不给这些权限。于是有了系统调用，危险的指令被包装成系统调用，用户程序只能调用而无权自己运行那些危险的指令。另外，计算机硬件的资源是有限的，为了更好的管理这些资源，所有的资源都由操作系统控制，进程只能向操作系统请求这些资源。操作系统是这些资源的唯一入口，这个入口就是系统调用。

系统调用是操作系统提供给应用程序的一种接口，用于请求操作系统内核执行特定的任务或访问受限的资源。它是应用程序与操作系统内核之间通信的主要途径。通过系统调用，应用程序可以执行以下几类操作：

1. **文件操作**：如打开、读取、写入、关闭文件，创建目录等。
2. **进程控制**：如创建新进程、终止进程、获取进程状态等。
3. **内存管理**：如申请、释放内存，改变内存保护属性等。
4. **设备输入输出**：如读取键盘输入，显示屏幕输出，控制打印机等。
5. **网络通信**：如建立套接字连接，发送和接收数据包等。
6. **系统信息查询**：如获取系统时间，查看系统状态等。

系统调用通常涉及从用户态（应用程序运行的状态）转换到核心态（内核运行的状态），这是一个特权级别较高的运行环境，允许执行底层硬件操作和访问受保护的系统资源。当应用程序发出系统调用请求时，CPU会切换到核心态，执行内核中的相应代码，完成请求后，再切换回用户态，将控制权返回给应用程序。

### 3.11.2 系统调用举例：
open和write都是系统调用。
还有写数据write，创建进程fork，vfork等都是系统调用。

系统调用的典型例子包括 `open`、`read`、`write`、`close`、`fork`、`exec`、`exit`、`mmap`、`munmap`、`ioctl`、`socket`、`bind`、`connect`、`listen`、`accept` 等。每个系统调用都有其特定的功能和参数格式。

在现代操作系统中，系统调用是通过特定的硬件中断机制或陷阱（trap）指令来实现的，以确保安全和效率。应用程序通过调用库函数或直接使用汇编指令来触发系统调用。系统调用的具体实现和可用的系统调用列表可能因不同的操作系统而异。

## 3.12 请你来说一说用户态到内核态的转化原理
### 3.12.1 用户态切换到内核态的3种方式
#### 3.12.1.1、系统调用
这是用户进程主动要求切换到内核态的一种方式，用户进程通过系统调用申请操作系统提供的服务程序完成工作。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的ine 80h中断。
#### 3.12.1.2、异常
当CPU在执行运行在用户态的程序时，发现了某些事件不可知的异常，这是会触发由当前运行进程切换到处理此。异常的内核相关程序中，也就到了内核态，比如缺页异常。
#### 3.12.1.3、外围设备的中断
当外围设备完成用户请求的操作之后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条将要执行的指令，转而去执行中断信号的处理程序，如果先执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了有用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。

### 3.12.2 用户态切换到内核态 切换操作
CPU中有一个标志字段，标志着线程的运行状态。用户态和内核态对应着不同的值，**用户态为3，内核态为0.**

**每个线程**都对应着一个**用户栈和内核栈**，分别用来执行**用户方法和内核方法**。
用户方法就是普通的操作。
内核方法就是访问磁盘、内存分配、网卡、声卡等敏感操作。

当用户**尝试调用内核方法**的时候，就会发生用户态切换到内核态的转变。

1用户态进程通过一个特殊的 CPU 指令（例如，在 x86 架构中的 int 指令）引发一个软件中断。
2软件中断使 CPU 切换到内核态，并跳转到预定的内核代码。
3内核代码（即系统调用的处理代码）执行请求的操作。
4当操作完成后，内核代码使用另一种特殊的 CPU 指令（例如，在 x86 架构中的 iret 指令）返回到用户态，并跳转回原来的用户代码。
整个过程中，操作系统内核负责保存和恢复用户进程的状态，以使用户进程不会意识到这种模式的切换。

切换流程：
1、每个线程都对应这一个TCB，**TCB中有一个TSS字段**，存储着**线程对应的内核栈的地址**，也就是**内核栈的栈顶指针**。
2、因为从用户态切换到内核态时，需要将**用户态对应的CPU现场信息保存起来**啊，这些信息对应着当前**用户栈的执行状态**，也就是当前用户态执行到哪一步了，方便后续的恢复。
所以会将CPU中关于当前用户栈的信息，也就是**各类寄存器信息**，包括**PC寄存器**信息写入到**内核栈**中，方便后续内核方法调用完毕后，**恢复用户方法执行的现场**。
3、将CPU的字段改为**内核态**，将**内核段对应的代码地址写入到PC寄存器**中，然后**开始执行内核方法**，相应的方法栈帧时保存在**内核栈**中。
4、当内核方法执行完毕后，会将**CPU的字段改为用户态**，然后利用之前写入的信息来**恢复用户栈的执行**。
从上述流程可以看出用户态切换到内核态的时候，会牵扯到用户态现场信息的保存以及恢复，还要进行一系列的安全检查，比较耗费资源。

下面是一些可能的改变：

1.`Program Counter(PC)`：程序计数器保存着线程需要下一步执行的指令的地址。在系统调用发生时，PC会被改变以指向系统调用的处理代码。当系统调用结束后，PC会被恢复为原来的值，以便线程可以继续执行用户态代码。
2.`Processor State`：这部分保存了线程的一些状态信息，包括寄存器的值，优先级，以及其他状态信息。在进行系统调用时，操作系统会保存这些信息以便在系统调用结束后可以恢复线程的状态。
3.`Process State`：线程的状态（例如，运行，就绪，阻塞等）可能会在进行系统调用时发生改变。例如，如果一个线程发出了一个读取文件的系统调用，那么在文件读取完成之前，这个线程可能会被标记为阻塞状态。

从出发方式看，可以在认为存在前述3种不同的类型，但是从最终实际完成由用户态到内核态的切换操作上来说，涉及的关键步骤是完全一样的，没有任何区别，都相当于执行了一个中断响应的过程，因为系统调用实际上最终是中断机制实现的，而异常和中断处理机制基本上是一样的，用户态切换到内核态的步骤主要包括：
1、从当前进程的描述符中提取其内核栈的ss0及esp0信息。
2、使用ss0和esp0指向的内核栈将当前进程的cs,eip，eflags，ss,esp信息保存起来，这个过程也完成了由用户栈找到内核栈的切换过程，同时保存了被暂停执行的程序的下一条指令。
3、将先前由中断向量检索得到的中断处理程序的cs，eip信息装入相应的寄存器，开始执行中断处理程序，这时就转到了内核态的程序执行了。

### 3.13 请你来说一下微内核与宏内核
#### 3.13.1 宏内核：
除了最基本的进程、线程管理、内存管理外，将文件系统，驱动，网络协议等等都集成在内核里面，例如linux内核。
#### 3.13.1.1 优点：
效率高。
#### 3.13.1.2 缺点：
稳定性差，开发过程中的bug经常会导致整个系统挂掉。

#### 3.13.2 微内核：
内核中只有最基本的调度、内存管理。驱动、文件系统等都是用户态的守护进程去实现的。
#### 3.13.2.1 优点：
稳定，驱动等的错误只会导致相应进程死掉，不会导致整个系统都崩溃
#### 3.13.2.2 缺点：
效率低。典型代表QNX，QNX的文件系统是跑在用户态的进程，称为resmgr的东西，是订阅发布机制，文件系统的错误只会导致这个守护进程挂掉。不过数据吞吐量就比较不乐观了。

## 3.14 僵尸进程
### 3.14.1 正常进程
正常情况下，子进程是通过父进程创建的，子进程再创建新的进程。子进程的结束和父进程的运行是一个异步过程，即父进程永远无法预测子进程到底什么时候结束。 当一个进程完成它的工作终止之后，它的父进程需要调用wait()或者waitpid()系统调用取得子进程的终止状态。
unix提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息， 就可以得到：在每个进程退出的时候，内核释放该进程所有的资源，包括打开的文件，占用的内存等。 但是仍然为其保留一定的信息，直到父进程通过wait / waitpid来取时才释放。保存信息包括：

1. 进程号the process ID
2. 退出状态the termination status of the process
3. 运行时间the amount of CPU time taken by the process等

### 3.14.2 孤儿进程
一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。
### 3.14.3 僵尸进程
一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵尸进程。
僵尸进程是一个进程必然会经过的过程：这是每个子进程在结束时都要经过的阶段。
如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“Z”。如果父进程能及时 处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。
如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。

### 3.14.4 僵尸进程危害：
如果进程不调用wait / waitpid的话， 那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程。
### 3.14.5 僵尸进程外部消灭：
通过kill发送SIGTERM或者SIGKILL信号消灭产生僵尸进程的进程，它产生的僵死进程就变成了孤儿进程，这些孤儿进程会被init进程接管，init进程会wait()这些孤儿进程，释放它们占用的系统进程表中的资源
### 3.14.6 僵尸进程内部解决：
1. 子进程退出时向父进程发送SIGCHILD信号，父进程处理SIGCHILD信号。在信号处理函数中调用wait进行处理僵尸进程。
2. fork两次，原理是将子进程成为孤儿进程，从而其的父进程变为init进程，通过init进程可以处理僵尸进程。
## 3.15 零拷贝
零拷贝（Zero-copy）是一种数据传输技术，它的核心理念在于减少数据在不同内存区域之间的复制次数，尤其是避免在用户空间（应用程序所在）和内核空间（操作系统内核所在）之间的数据复制，以此来提高数据传输的效率和系统的性能。

在传统的I/O操作中，数据往往需要经历多次复制，比如从磁盘读取数据到内核缓冲区，然后从内核缓冲区复制到用户空间的缓冲区，最后可能还要从用户空间的缓冲区复制到网络套接字的缓冲区才能发送出去。这些额外的复制不仅消耗CPU周期，还增加了内存带宽的使用，降低了整体的系统性能。
零拷贝技术通过以下几种方式来减少或消除这些不必要的数据复制：
1. **直接I/O**：数据从磁盘读取后直接进入用户空间的缓冲区，跳过内核缓冲区。
2. **内存映射文件**（Memory-Mapped Files）：使用mmap系统调用将文件映射到内存地址空间，这样应用程序可以直接操作内存中的数据，就像操作普通数组一样，而无需显式读取或写入文件。
3. **I/O多路复用**：通过select、poll、epoll等I/O多路复用机制，可以让多个I/O操作共享一个线程，避免了频繁的上下文切换和不必要的数据复制。
4. **Sendfile系统调用**：sendfile直接从一个文件描述符（通常是一个磁盘文件）读取数据并发送到另一个文件描述符（通常是一个网络套接字），而不需要将数据复制到用户空间。
5. **Direct Memory Access (DMA)**：硬件级别的直接内存访问，允许数据直接从网络适配器或磁盘控制器移动到内存，而无需CPU干预。
通过这些机制，零拷贝技术可以显著减少数据处理的延迟，降低CPU和内存的开销，提高数据传输的速度和系统的整体性能。在现代操作系统和高性能网络应用中，零拷贝技术是优化数据传输和I/O性能的重要手段之一。
# 四、 C++线程
### 4.1 创建线程调用方式
#### 4.1.1 函数指针
```cpp
// 1.函数指针
void fun(int x) {
    while (x-- > 0) {
        cout << x << endl;
    }
}
// 调用
std::thread t1(fun, 10);
t1.join();
```
#### 4.1.2 Lambda函数
```cpp
// 注意：如果我们创建多线程 并不会保证哪一个先开始
int main() {
    // 2.Lambda函数
    auto fun = [](int x) {
        while (x-- > 0) {
            cout << x << endl;
        }
    };
//    std::1.thread t1(fun, 10);
    // 也可以写成下面：
    std::thread t1_1([](int x) {
        while (x-- > 0) {
            cout << x << endl;
        }
    }, 11);
//    std::1.thread t2(fun, 10);
//    t1.join();
    t1_1.join();
//    t2.join();
    return 0;
}
```
#### 4.1.3 仿函数
```cpp
// 3.functor (Funciton Object)
class Base {
public:
    void operator()(int x) {
        while (x-- > 0) {
            cout << x << endl;
        }
    }
};
// 调用
thread t(Base(), 10);
t.join();
```
#### 4.1.4 非静态成员函数
```c++
// 4.Non-static member function
class Base {
public:
    void fun(int x) {
        while (x-- > 0) {
            cout << x << endl;
        }
    }
};
// 调用
thread t(&Base::fun,&b, 10);
t.join();
```
#### 4.1.5 静态成员函数
```cpp
// 5.static member function
class Base {
public:
    static void fun(int x) {
        while (x-- > 0) {
            cout << x << endl;
        }
    }
};
// 调用
thread t(&Base::fun, 10);
t.join();
```
### 4.2 mutex互斥量
参见 ：
[c++之多线程中“锁”的基本用法
](https://zhuanlan.zhihu.com/p/91062516)
[C++多线程开发之互斥锁](https://light-city.club/sc/concurrency/Threading_In_CPlusPlus/thread/#2)

#### 4.2.1 mutex无锁情况
```cpp
#include <iostream>
#include <mutex>
#include <thread>

using namespace std;

int sum = 0; //shared

void *countgold()
{
    int i; //local to each thread
    for (i = 0; i < 10000000; i++) {
        sum += 1;
    }
    return NULL;
}

int main()
{
    thread t1(countgold);
    thread t2(countgold);

    //Wait for both threads to finish
    t1.join();
    t2.join();

    cout << "sum = " << sum << endl;
    return 0;
}
➜  thread ./mutex_demo1_no_mutex
sum = 19131716
➜  thread ./mutex_demo1_no_mutex
sum = 20000000
➜  thread ./mutex_demo1_no_mutex
sum = 18729155
```
#### 4.2.2 使用mutex加锁
```cpp
#include <iostream>
#include <thread>
#include <vector>
#include <mutex>
#include <chrono>
#include <stdexcept>

int sum = 0; //shared
int counter = 0;
std::mutex mtx;

void increase(int time)
{
    for (int i = 0; i < time; i++)
    {
        mtx.lock();
        counter+=1;
        mtx.unlock();
    }
}

void *countgold()
{
    int i; //local to each thread
    for (i = 0; i < 10000000; i++) {
        sum += 1;
    }
    return NULL;
}

int main(int argc, char** argv)
{
    std::cout << "sum = " << sum << std::endl;
    std::cout << "counter:" << counter << std::endl;
    std::thread t1(increase, 10000000);
    std::thread t2(increase, 10000000);
    t1.join();
    t2.join();

    std::thread t3(countgold);
    std::thread t4(countgold);

    //Wait for both threads to finish
    t3.join();
    t4.join();

    std::cout << "sum = " << sum << std::endl;
    std::cout << "counter:" << counter << std::endl;
    return 0;
}
➜  thread ./mutex_demo2_with_mutex
sum = 0
counter:0
sum = 19366730
counter:20000000
➜  thread ./mutex_demo2_with_mutex
sum = 0
counter:0
sum = 19666435
counter:20000000
```
#### 4.2.3 总结使用mutex

1. 对于std::mutex对象，任意时刻最多允许一个线程对其进行上锁
2. mtx.lock()：调用该函数的线程尝试加锁。如果上锁不成功，即：其它线程已经上锁且未释放，则当前线程block。如果上锁成功，则执行后面的操作，操作完成后要调用mtx.unlock()释放锁，否则会导致死锁的产生
3. mtx.unlock()：释放锁
4. std::mutex还有一个操作：mtx.try_lock()，字面意思就是：“尝试上锁”，与mtx.lock()的不同点在于：如果上锁不成功，当前线程不阻塞。

## 4.3 lock_guard
### 4.3.1 死锁
```cpp
#include <functional>
#include <iostream>
#include <thread>
#include <vector>
#include <mutex>
#include <chrono>
#include <stdexcept>

int counter  = 0;
int counter2 = 0;
std::mutex mtx;

#define TEST 1
#if TEST
void increase(int time)
{
    for (int i = 0; i < time; i++)
    {
        mtx.lock();
        counter+=1;
        mtx.unlock();
    }
}
#endif

void increase_proxy(int time, int id)
{
    for (int i = 0; i < time; i++)
    {
        mtx.lock();
        // 线程1上锁成功后，抛出异常：未释放锁
        if (id == 1)
        {
            throw std::runtime_error("throw excption....");
        }
        counter2++;
        mtx.unlock();
    }
}
void increase(int time, int id)
{
    try
    {
        for (int i = 0; i < time; i++)
        {
            mtx.lock();
            // 线程1上锁成功后，抛出异常：未释放锁
            if (id == 1)
            {
                throw std::runtime_error("throw excption....");
            }
            counter2++;
            mtx.unlock();
        }
        //increase_proxy(time, id);
    }
    catch (const std::exception& e)
    {
        std::cout << "id:" << id << ", " << e.what() << std::endl;
    }
}

int main(int argc, char** argv)
{
#if TEST
    std::cout << "counter:" << counter << std::endl;
//    auto func = increace();
    std::function<void(int)>     f1 = [] (int a) { return increase(a); };
    std::function<void(int,int)> f2 = [] (int a,int b) { return increase(a,b); };

    std::thread t1(f1, 10000000);
    std::thread t2(f1, 10000000);
    t1.join();
    t2.join();
    std::cout << "counter:" << counter << std::endl;
#endif

    std::cout << "counter2:" << counter2 << std::endl;
    std::thread t3(f2, 10000000, 1);
    std::thread t4(f2, 10000000, 2);
    t3.join();
    t4.join();

    std::cout << "counter2:" << counter2 << std::endl;
    return 0;
}

➜  thread ./mutex_demo3_dead_lock
counter:0
counter:20000000
counter2:0
id:1, throw excption....
```
- 没有退出，发生死锁
- std::lock_guard只有构造函数和析构函数。
    * 简单的来说：当调用构造函数时，会自动调用传入的对象的lock()函数，而当调用析构函数时，自动调用unlock()函数（这就是所谓的RAII，读者可自行搜索）。

### 4.3.2 lock_guard 避免死锁
```cpp
#include <functional>
#include <iostream>
#include <thread>
#include <vector>
#include <mutex>
#include <chrono>
#include <stdexcept>

int counter  = 0;
int counter2 = 0;
std::mutex mtx;

void increase(int time)
{
    for (int i = 0; i < time; i++)
    {
        counter+=1;
    }
}

void increase(int time, int id)
{
    try
    {
        for (int i = 0; i < time; i++)
        {
            // 不再使用 mtx.lock();替换为 lock_guard
            // std::lock_guard对象构造时，自动调用mtx.lock()进行上锁
            // std::lock_guard对象析构时，自动调用mtx.unlock()释放锁
            std::lock_guard<std::mutex> lk(mtx);
            // 线程1上锁成功后，抛出异常：未释放锁
            if (id == 1)
            {
                throw std::runtime_error("throw excption....");
            }
            counter2++;
        }
    }
    catch (const std::exception& e)
    {
        std::cout << "id:" << id << ", " << e.what() << std::endl;
    }
}

int main(int argc, char** argv)
{
    std::cout << "counter:" << counter << std::endl;
    std::function<void(int)>     f1 = [] (int a) { return increase(a); };
    std::function<void(int,int)> f2 = [] (int a,int b) { return increase(a,b); };

    std::thread t1(f1, 10000000);
    std::thread t2(f1, 10000000);
    t1.join();
    t2.join();
    std::cout << "counter:" << counter << std::endl;

    std::cout << "counter2:" << counter2 << std::endl;
    std::thread t3(f2, 10000000, 1);
    std::thread t4(f2, 10000000, 2);
    t3.join();
    t4.join();

    std::cout << "counter2:" << counter2 << std::endl;
    return 0;
}

➜  thread ./mutex_demo4_lock_guard
counter:0
counter:15638387
counter2:0
id:1, throw excption....
counter2:10000000
```
- 结果符合预期。所以，推荐使用std::mutex和std::lock_guard搭配使用，避免死锁的发生。

# 五、 pthread线程

### 5.1 pthread线程 mutex互斥量
```
#include <pthread.h>
#include <thread>
```
pthread 是 POSIX 线程库，提供了在多线程环境下进行线程管理和同步的函数。mutex（互斥量）是 pthread 库中用来实现线程同步的一种机制。
互斥量用于保护共享资源，确保在任意给定的时间只有一个线程可以访问该资源。它提供了两个主要的操作：lock（加锁）和 unlock（解锁）。
下面是一个使用 pthread 和互斥量的简单示例代码：
```cpp
#include <iostream>
#include <pthread.h>

// 共享资源
int sharedVariable = 0;

// 互斥量
pthread_mutex_t mutex;

// 线程函数
void* threadFunction(void* arg) {
    // 加锁
    pthread_mutex_lock(&mutex);

    // 访问共享资源
    sharedVariable++;
    std::cout << "Thread incremented sharedVariable: " << sharedVariable << std::endl;

    // 解锁
    pthread_mutex_unlock(&mutex);

    return nullptr;
}

int main() {
    // 初始化互斥量
    pthread_mutex_init(&mutex, nullptr);

    // 创建线程
    pthread_t thread;
    pthread_create(&thread, nullptr, threadFunction, nullptr);

    // 主线程等待子线程结束
    pthread_join(thread, nullptr);

    // 销毁互斥量
    pthread_mutex_destroy(&mutex);

    return 0;
}
```
此外，依据同一线程是否能多次加锁，把互斥量又分为如下两类：

是：称为『递归互斥量』recursive mutex ，也称『可重入锁』reentrant lock
否：即『非递归互斥量』non-recursive mute），也称『不可重入锁』non-reentrant mutex

C++ 递归互斥量的API： std::recursive_mutex
pthread则可以通过给mutex添加 PTHREAD_MUTEX_RECURSIVE

### 5.2 condition variable（条件变量）
C++11中也有条件变量的API： std::condition_variable。
pthread 条件变量 pthread_cond_t
条件变量（condition variable）是一种线程同步的机制，常用于多个线程之间的等待和通知。它允许一个或多个线程在某个条件满足之前进入等待状态，直到其他线程通过发出信号（signal）或广播（broadcast）来通知它们条件已经满足。

条件变量通常与互斥量一起使用，以实现更复杂的线程同步模式。下面是一个简单的示例代码，展示了条件变量的基本用法：

```cpp
#include <iostream>
#include <pthread.h>

// 共享资源
int sharedVariable = 0;

// 互斥量和条件变量
pthread_mutex_t mutex;
pthread_cond_t condition;

// 生产者线程函数
void* producerThread(void* arg) {
    // 加锁互斥量
    pthread_mutex_lock(&mutex);

    // 修改共享资源
    sharedVariable = 42;
    std::cout << "Producer thread: Set sharedVariable to 42." << std::endl;

    // 发出信号通知等待的线程
    pthread_cond_signal(&condition);

    // 解锁互斥量
    pthread_mutex_unlock(&mutex);

    return nullptr;
}

// 消费者线程函数
void* consumerThread(void* arg) {
    // 加锁互斥量
    pthread_mutex_lock(&mutex);

    // 等待条件满足
    while (sharedVariable == 0) {
        pthread_cond_wait(&condition, &mutex);
    }

    // 条件满足，处理共享资源
    std::cout << "Consumer thread: sharedVariable is now " << sharedVariable << std::endl;

    // 解锁互斥量
    pthread_mutex_unlock(&mutex);

    return nullptr;
}

int main() {
    // 初始化互斥量和条件变量
    pthread_mutex_init(&mutex, nullptr);
    pthread_cond_init(&condition, nullptr);

    // 创建生产者线程和消费者线程
    pthread_t producer, consumer;
    pthread_create(&producer, nullptr, producerThread, nullptr);
    pthread_create(&consumer, nullptr, consumerThread, nullptr);

    // 等待线程结束
    pthread_join(producer, nullptr);
    pthread_join(consumer, nullptr);

    // 销毁互斥量和条件变量
    pthread_mutex_destroy(&mutex);
    pthread_cond_destroy(&condition);

    return 0;
}
```
在上述示例中，我们定义了一个共享变量 sharedVariable，并创建了一个互斥量 mutex 和一个条件变量 condition。在生产者线程函数中，它会先修改共享变量为 42，然后通过 pthread_cond_signal 发出信号通知等待的线程。在消费者线程函数中，它会等待条件满足，即共享变量不为 0，如果条件不满足，则调用 pthread_cond_wait 进入等待状态。一旦条件满足，消费者线程就会处理共享资源。
在 C++11 中，引入了新的标准库 <condition_variable>，其中包含了条件变量的实现。C++11 的条件变量与之前提到的 POSIX 条件变量有一些差异，它们提供了更方便和类型安全的接口。

下面是一个示例代码，展示了如何在 C++11 中使用条件变量：

```cpp
#include <iostream>
#include <thread>
#include <mutex>
#include <condition_variable>

// 共享资源
int sharedVariable = 0;

// 互斥量和条件变量
std::mutex mutex;
std::condition_variable condition;

// 生产者线程函数
void producerThread() {
    // 加锁互斥量
    std::unique_lock<std::mutex> lock(mutex);

    // 修改共享资源
    sharedVariable = 42;
    std::cout << "Producer thread: Set sharedVariable to 42." << std::endl;

    // 发出信号通知等待的线程
    condition.notify_one();
}

// 消费者线程函数
void consumerThread() {
    // 加锁互斥量
    std::unique_lock<std::mutex> lock(mutex);

    // 等待条件满足
    condition.wait(lock, [] { return sharedVariable != 0; });

    // 条件满足，处理共享资源
    std::cout << "Consumer thread: sharedVariable is now " << sharedVariable << std::endl;
}

int main() {
    // 创建生产者线程和消费者线程
    std::thread producer(producerThread);
    std::thread consumer(consumerThread);

    // 等待线程结束
    producer.join();
    consumer.join();

    return 0;
}
```
在这个示例中，我们使用了 std::mutex 作为互斥量，std::condition_variable 作为条件变量。在生产者线程函数中，我们使用 std::unique_lock 对互斥量进行加锁，并修改了共享变量的值，然后通过 condition.notify_one() 发出信号通知等待的线程。在消费者线程函数中，我们使用 std::unique_lock 对互斥量进行加锁，并使用 condition.wait 等待条件满足。

需要注意的是，为了在 condition.wait 中指定等待的条件，我们使用了 lambda 表达式 [] { return sharedVariable != 0; }。这个 lambda 表达式返回一个 bool 值，表示条件是否满足。只有当条件不满足时，消费者线程才会等待。

C++11 的条件变量接口更加便利和类型安全，使用起来更加直观。
需要注意的是，在使用条件变量时，必须使用互斥量来保护共享资源的访问，并确保线程在等待条件时解锁互斥量。这样可以避免竞态条件和死锁。
[涛哥 线程间同步互斥（3）条件变量使用](https://zhuanlan.zhihu.com/p/136431212)
[飞翔的猪 再谈条件变量—从入门到出家](https://zhuanlan.zhihu.com/p/155547997)
[暗淡了乌云 条件变量 之 稀里糊涂的锁](https://zhuanlan.zhihu.com/p/55123862)
[好物推荐到火星 条件变量pthread API编程小练习](https://zhuanlan.zhihu.com/p/51387442)
[关于一点pthread_cond_t条件锁的思考以及实验](https://www.cnblogs.com/liulipeng/p/3552767.html)
[pthread_mutex_t 和 pthread_cond_t 配合使用的简要分析](https://blog.csdn.net/chengonghao/article/details/51779279)


使用 `while` 循环来检查条件变量是一种常见的做法，这通常被称为“等待-通知”模式。这样做可以防止虚假唤醒（spurious wakeups）和错过通知的情况。伪代码如下所示：

```cpp
std::unique_lock<std::mutex> lock(m);
while (!condition) {  // 注意这里使用的是 while 而不是 if
    cond_var.wait(lock);  // 等待条件满足
}
// 条件满足后执行相应操作
```

在这段代码中，使用 `while` 循环而不是 `if` 语句是为了确保线程在每次从 `wait` 返回时都会重新检查条件。即使线程被错误地唤醒或者错过了之前的通知，它也会继续等待直到条件真正成立。

**不会导致死循环**的原因是：
1. 如果 `condition` 为假，那么线程会调用 `cond_var.wait(lock)` 并释放锁，然后进入阻塞状态。
2. 当另一个线程改变了条件并调用了 `notify` 或 `notify_all` 之后，当前线程会被唤醒，并且会再次获取锁。
3. 在获取到锁之后，线程会再次检查 `condition`。如果此时条件已经变为真，那么 `while` 循环就会结束，程序将继续执行后面的代码。
4. 只有当条件一直不成立时，线程才会一直停留在 `wait` 中，而不会无休止地执行循环体。

重要的是要确保 `condition` 最终会被设置为真，否则线程确实会无限期地等待下去。因此，在多线程应用中正确地管理和更新条件是非常重要的。同时，修改 `condition` 的代码部分应该在持有相应的互斥锁的情况下进行，以保证条件的原子性和一致性。
### 5.3 read-write lock（读写锁）
共享-独占锁
读共享，写独占的锁
允许多个线程同时读数据，而写数据则必须保证没有其他线程在读写。

// 声明一个读写锁 pthread_rwlock_t | 静态分配的读写锁，可以用常值PTHREAD_\RWLOCK_INITIALIZER来初始化 | 调用pthread_rwlock_init来动态初始化
pthread_rwlockattr_setkind_np用来设置writer或者reader优先权，缺省设置是PTHREAD_RWLOCK_PREFER_READER_NP，也就是reader优先进入lock队列，这会造成writer饿死；
另外的设置是PTHREAD_RWLOCK_PREFER_WRITER_NONRECURSIVE_NP，它可以解决writer饿死的问题，具体实现是writer在获得lock前会阻止reader继续进入lock队列，解决writer的问题同时，它也会造成reader饿死可能。
C++17 std::shared_mutex 模拟实现出读写锁
```cpp
// 初始化读写锁
int pthread_rwlock_init(pthread_rwlock_t *restrict rwlock, const pthread_rwlockattr_t *restrict attr);
// 获得读锁，如果此时其他线程已经获得了写锁，那么会等待
int pthread_rwlock_rdlock(pthread_rwlock_t *rwlock);
// 获得写锁，如果此时其他线程已经获得读锁或写锁，那么会等待
int pthread_rwlock_wrlock(pthread_rwlock_t *rwlock);
// 释放读锁或写锁
int pthread_rwlock_unlock(pthread_rwlock_t *rwlock);
// 结束读写锁
int pthread_rwlock_destroy(pthread_rwlock_t *rwlock);
```
示例：
```cpp
#include <pthread.h>
#include <stdio.h>

// 声明一个读写锁
pthread_rwlock_t rwlock;

// 全局共享资源
int shared_data = 0;

// 线程函数A，用于读取共享资源
void* thread_read(void* arg) {
    pthread_rwlock_rdlock(&rwlock);  // 获取读锁（共享锁）

    // 读取共享资源
    printf("Reading data: %d\n", shared_data);

    pthread_rwlock_unlock(&rwlock);  // 释放读锁

    return NULL;
}

// 线程函数B，用于修改共享资源
void* thread_write(void* arg) {
    pthread_rwlock_wrlock(&rwlock);  // 获取写锁（独占锁）

    // 修改共享资源
    shared_data += 1;
    printf("Writing data: %d\n", shared_data);

    pthread_rwlock_unlock(&rwlock);  // 释放写锁

    return NULL;
}

int main() {
    // 初始化读写锁
    pthread_rwlock_init(&rwlock, NULL);

    pthread_t t1, t2, t3;

    // 创建线程A、B、C
    pthread_create(&t1, NULL, thread_read, NULL);
    pthread_create(&t2, NULL, thread_write, NULL);
    pthread_create(&t3, NULL, thread_read, NULL);

    // 等待线程执行完毕
    pthread_join(t1, NULL);
    pthread_join(t2, NULL);
    pthread_join(t3, NULL);

    // 销毁读写锁
    pthread_rwlock_destroy(&rwlock);

    return 0;
}
```
我们先声明了一个 `pthread_rwlock_t` 类型的读写锁 rwlock。
然后创建了两个线程函数：`thread_read` 用于读取共享资源，`thread_write` 用于修改共享资源。在 `main()` 函数中，我们初始化了读写锁，创建了三个线程，并等待它们执行完毕。最后，销毁了读写锁。

注意，在程序中使用读写锁时，需要使用 `pthread_rwlock_rdlock()` 获取读锁（共享锁），`pthread_rwlock_wrlock()` 获取写锁（独占锁），并使用 `pthread_rwlock_unlock()` 释放锁。

[线程间同步互斥（2）读写锁使用 涛哥](https://zhuanlan.zhihu.com/p/135983375)
在C++11及以后的版本中，你可以使用`std::shared_mutex（共享互斥量）`来实现读写锁。该互斥量允许多个线程以共享方式进行读取，并对写操作进行独占。
通过使用`std::shared_lock`和`std::unique_lock`可以实现对std::shared_mutex的读写锁操作。
`std::shared_lock`用于获取共享锁（读锁），允许多个线程同时读取共享资源。多个`std::shared_lock`可以同时获得读锁，提供了共享访问的能力。读锁之间不会互相阻塞，因此多个线程可以并发地读取共享数据。
`std::unique_lock`用于获取独占锁（写锁），确保只有一个线程可以修改共享资源。在任何时候只能有一个std::unique_lock获得写锁，这样可以避免多个线程同时修改数据造成的竞争条件。
```cpp
#include <iostream>
#include <mutex>
#include <shared_mutex>
#include <thread>

std::shared_mutex rwlock;  // 声明一个读写锁

int shared_data = 0;  // 共享资源

void thread_read() {
    std::shared_lock<std::shared_mutex> lock(rwlock);  // 获取读锁（共享锁）

    // 读取共享资源
    std::cout << "Reading data: " << shared_data << std::endl;

    // 读锁会自动释放，不需要手动调用unlock()
}

void thread_write() {
    std::unique_lock<std::shared_mutex> lock(rwlock);  // 获取写锁（独占锁）

    // 修改共享资源
    shared_data += 1;
    std::cout << "Writing data: " << shared_data << std::endl;

    // 写锁会自动释放，不需要手动调用unlock()
}

int main() {
    std::thread t1(thread_read);
    std::thread t2(thread_write);
    std::thread t3(thread_read);

    t1.join();
    t2.join();
    t3.join();

    return 0;
}
```
我们使用了`std::shared_mutex`来实现读写锁的功能。通过`std::shared_lock`获取读锁（共享锁），允许多个线程同时读取共享资源。通过`std::unique_lock`获取写锁（独占锁），确保只有一个线程可以修改共享资源。

### 5.4 spinlock（自旋锁）
『忙等待』（busy waiting） 死循环
当共享资源的状态不满足的时候，自旋锁会不停地循环检测状态.因为不会陷入休眠，而是忙等待的方式也就不需要条件变量。
不休眠就不会引起上下文切换，但是会比较浪费CPU。
在中断上下文，是不允许睡眠的，所以，这里需要的是一个不会导致睡眠的锁——spinlock。
换言之，中断上下文要用锁，首选 spinlock。

pthread_spin_init 函数的第二个参数名为pshared（int类型）
- PTHREAD_PROCESS_PRIVATE：仅同进程下读线程可以使用该自旋锁
- PTHREAD_PROCESS_SHARED：不同进程下的线程可以使用该自旋锁
pthread库提供了自旋锁（spinlock）机制来实现线程间的互斥访问。自旋锁是一种忙等待的锁，当线程尝试获取锁时，如果锁已经被其他线程占用，该线程会进入一个忙循环，不断地进行自旋等待，直到获取到锁为止。

在pthread库中，自旋锁通过`pthread_spinlock_t`类型的变量来表示。下面是使用自旋锁的简单示例：

```cpp
#include <iostream>
#include <pthread.h>

pthread_spinlock_t spinlock;  // 自旋锁

int shared_data = 0;  // 共享资源

void* thread_func(void* arg) {
    pthread_spin_lock(&spinlock);  // 获取自旋锁

    // 修改共享资源
    shared_data += 1;
    std::cout << "Thread ID: " << pthread_self() << ", Writing data: " << shared_data << std::endl;

    pthread_spin_unlock(&spinlock);  // 释放自旋锁

    return nullptr;
}

int main() {
    pthread_spin_init(&spinlock, PTHREAD_PROCESS_PRIVATE);  // 初始化自旋锁

    pthread_t tid1, tid2;
    pthread_create(&tid1, nullptr, thread_func, nullptr);
    pthread_create(&tid2, nullptr, thread_func, nullptr);

    pthread_join(tid1, nullptr);
    pthread_join(tid2, nullptr);

    pthread_spin_destroy(&spinlock);  // 销毁自旋锁

    return 0;
}
```
在这个例子中，我们使用pthread_spinlock_t类型的变量spinlock来表示自旋锁。通过调用pthread_spin_lock函数获取锁，并在获取到锁后修改共享资源。最后通过pthread_spin_unlock函数释放锁。

需要注意的是，自旋锁适用于短期占用场景，当占用时间较长或线程数量较多时，自旋锁可能会导致CPU资源的浪费，因为线程会一直自旋等待。在实际使用中，需根据具体情况选择合适的锁机制。

在 C++11 中，并没有直接提供自旋锁（spin lock）的标准库支持。自旋锁是一种非阻塞式的锁，它使用忙等（busy-waiting）的方式来实现线程间的同步，即线程在获取锁失败时会不断循环尝试获取锁。

然而，虽然 C++11 标准库没有提供自旋锁的支持，但可以使用原子操作和条件变量来手动实现自旋锁。

下面是一个简单的示例代码，展示了如何使用原子操作和条件变量来实现自旋锁：

```cpp
#include <iostream>
#include <thread>
#include <atomic>
#include <condition_variable>

class SpinLock {
public:
    void lock() {
        while (flag.test_and_set(std::memory_order_acquire)) {
            // 自旋等待
        }
    }

    void unlock() {
        flag.clear(std::memory_order_release);
    }

private:
    std::atomic_flag flag = ATOMIC_FLAG_INIT;
};

// 共享资源
int sharedVariable = 0;

// 自旋锁
SpinLock spinLock;

// 生产者线程函数
void producerThread() {
    // 获取锁
    spinLock.lock();

    // 修改共享资源
    sharedVariable = 42;
    std::cout << "Producer thread: Set sharedVariable to 42." << std::endl;

    // 释放锁
    spinLock.unlock();
}

// 消费者线程函数
void consumerThread() {
    // 获取锁
    spinLock.lock();

    // 处理共享资源
    std::cout << "Consumer thread: sharedVariable is now " << sharedVariable << std::endl;

    // 释放锁
    spinLock.unlock();
}

int main() {
    // 创建生产者线程和消费者线程
    std::thread producer(producerThread);
    std::thread consumer(consumerThread);

    // 等待线程结束
    producer.join();
    consumer.join();

    return 0;
}
```
在这个示例中，我们定义了一个简单的自旋锁 SpinLock，它使用了 `std::atomic_flag` 来实现原子的测试和设置操作。在 SpinLock 的 lock 方法中，使用了 `test_and_set` 操作来尝试获取锁，如果获取失败就会一直循环等待。在 SpinLock 的 unlock 方法中，使用了 clear 操作来释放锁。

然后，我们在生产者线程函数和消费者线程函数中使用了自旋锁来保护共享资源的访问。

flag 是一个 `std::atomic_flag` 类型的对象，它用于表示自旋锁的状态。`std::atomic_flag` 是一个原子标志类型，提供了原子的测试和设置操作。
`test_and_set()` 函数是 `std::atomic_flag` 类型的成员函数，它会原子地将 flag 设置为 true，并返回设置之前的值。在这里，我们使用 `test_and_set()` 函数来尝试获取自旋锁。
`std::memory_order_acquire` 是一个内存排序参数，它的作用是确保在获取自旋锁时所有的读操作都发生在获取操作之前。这样可以保证获取操作之后对共享数据的读取是正确的。
当 `test_and_set()` 返回 true，即当前自旋锁已被其他线程占用时，进入 while 循环，执行自旋等待。自旋等待是简单的循环操作，它会一直尝试获取自旋锁直到成功。
需要注意的是，自旋锁在一些情况下可能会导致高额的 CPU 开销，因为线程会一直处于忙等状态。因此，在实际使用时，需要根据具体情况来判断是否适合使用自旋锁。

**`std::atomic_flag`** 是一个特殊的原子类型，它只提供了最基础的布尔标志功能，而且仅支持两种原子操作：
1. `test_and_set`：测试并设置标志位，如果标志位之前没有被设置，那么设置它并返回 `false`；如果已经被设置，那么不做任何改变并返回 `true`。
2. `clear`：清除标志位，将其重置为未设置状态。
`std::atomic_flag` 的设计非常简单，没有复制构造函数和赋值运算符，这意味着它不能被复制。这是为了防止在多线程环境中由于复制导致的潜在竞态条件。

**`std::atomic<T>`** 则是一个通用的原子类型模板，它可以应用于多种数据类型，如 `int`, `long`, `bool`, 指针等（但不包括浮点数和复合数据类型）。`std::atomic` 提供了一系列原子操作，包括但不限于：

- `load`：加载原子变量的值。
- `store`：存储一个值到原子变量。
- `exchange`：交换原子变量的值，并返回旧值。
- `compare_exchange_weak`：弱比较并交换，如果当前值等于期望值，则替换为新值。
- `compare_exchange_strong`：强比较并交换，与弱版本类似，但在失败时具有更强的保证。
- `fetch_add`, `fetch_sub`, `fetch_and`, `fetch_or`, `fetch_xor`：获取并应用一些算术或逻辑操作。

`std::atomic<bool>` 是 `std::atomic` 模板的一个特例，它提供了一个布尔型的原子变量，相比 `std::atomic_flag` 更加灵活和强大，因为你可以直接用 `true` 或 `false` 初始化它，并且可以使用上述提到的所有原子操作。

总之，`std::atomic_flag` 是一个轻量级的、专门用于标志位的原子类型，而 `std::atomic<T>` 是一个更为通用和强大的原子类型模板，适用于更广泛的原子操作场景。

### 5.5 信号量(semaphore)
```cp
#include <semaphore.h>
// 初始化信号量
int sem_init(sem_t *sem, int pshared, unsigned int value);
// 信号量 P 操作（减 1）
int sem_wait(sem_t *sem);
// 以非阻塞的方式来对信号量进行减 1 操作
int sem_trywait(sem_t *sem);
// 信号量 V 操作（加 1）
int sem_post(sem_t *sem);
// 获取信号量的值
int sem_getvalue(sem_t *sem, int *sval);
// 销毁信号量
int sem_destroy(sem_t *sem);
```
```cpp
#include <iostream>
#include <thread>
#include <semaphore.h>

sem_t semaphore; // 信号量

void WorkerTask(int id) {
    sem_wait(&semaphore); // 等待信号量可用

    std::cout << "Worker " << id << " is working!" << std::endl;
    std::this_thread::sleep_for(std::chrono::seconds(2));

    sem_post(&semaphore); // 释放信号量
    std::cout << "Worker " << id << " finished!" << std::endl;
}

int main() {
    sem_init(&semaphore, 0, 3); // 初始化信号量，初始值为3

    std::thread workers[5];
    for (int i = 0; i < 5; ++i) {
        workers[i] = std::thread(WorkerTask, i + 1);
    }

    for (int i = 0; i < 5; ++i) {
        workers[i].join();
    }

    sem_destroy(&semaphore); // 销毁信号量

    return 0;
}
```
我们使用了sem_t类型的变量semaphore来表示信号量。通过`sem_init()`函数初始化信号量，其中第二个参数表示是否在进程间共享，最后一个参数表示信号量的初始值。

在WorkerTask()函数中，线程首先调用`sem_wait()`函数等待信号量可用，然后执行任务，最后调用sem_post()函数释放信号量。

在main()函数中，我们创建了5个线程来执行任务。通过sem_init()初始化信号量，并指定初始值为3，表示最多同时有3个线程可以获得信号量。然后，每个线程在开始前调用`sem_wait()`等待信号量，获得信号量后执行任务，最后调用`sem_post()`释放信号量。最后，通过`sem_destroy()`销毁信号量。

semaphore.h头文件提供了一些函数来操作信号量，如`sem_init()`、`sem_wait()`、`sem_post()`和`sem_destroy()`等。


[详解linux多线程——互斥锁、条件变量、读写锁、自旋锁、信号量](https://zhuanlan.zhihu.com/p/161010435)
信号量（Semaphore）是一种经典的同步机制，用于控制对共享资源的访问。它可以用来解决多线程或多进程之间的互斥和同步问题。

在C++中，我们可以使用std::mutex和std::condition_variable来实现信号量。

下面是一个使用信号量的简单示例：

```cpp
#include <iostream>
#include <mutex>
#include <condition_variable>
#include <thread>

class Semaphore {
public:
    explicit Semaphore(int count = 0) : count_(count) {}

    void Acquire() {
        std::unique_lock<std::mutex> lock(mutex_);
        while (count_ == 0) {
            condition_.wait(lock);
        }
        --count_;
    }

    void Release() {
        std::unique_lock<std::mutex> lock(mutex_);
        ++count_;
        condition_.notify_one();
    }

private:
    std::mutex mutex_;
    std::condition_variable condition_;
    int count_;
};

Semaphore semaphore(2);  // 信号量，初始值为2

void thread_func(int id) {
    semaphore.Acquire();  // 获取信号量

    // 访问共享资源
    std::cout << "Thread " << id << " is accessing the shared resource." << std::endl;

    semaphore.Release();  // 释放信号量
}

int main() {
    std::thread t1(thread_func, 1);
    std::thread t2(thread_func, 2);
    std::thread t3(thread_func, 3);

    t1.join();
    t2.join();
    t3.join();

    return 0;
}
```
在这个例子中，我们定义了一个Semaphore类，其内部包含一个计数器count_、一个互斥量mutex_和一个条件变量condition_。Acquire()方法用于获取信号量，如果计数器为零，则线程会进入等待状态；Release()方法用于释放信号量，将计数器加一，并通知一个正在等待的线程。

在main()函数中创建了三个线程，它们会调用thread_func()函数来访问共享资源。由于信号量的初始值为2，因此前两个线程可以成功获取信号量并访问共享资源，第三个线程需要等待其中之一释放信号量后才能获取。

通过使用信号量，我们可以控制对共享资源的并发访问，实现线程间的同步和互斥。
### 5.6 python的锁
Python的锁，常用的有以下几种：

线程锁（Lock）：threading.Lock()函数返回一个普通锁对象，可以使用acquire()方法获取锁，release()方法释放锁。它可以用于线程之间的同步，确保在同一时间只有一个线程可以访问共享资源。

递归锁（RLock）：threading.RLock()函数返回一个可重入锁对象，可以被一个线程多次获取。与普通锁相比，递归锁允许同一个线程多次获取锁而不会导致死锁。需要注意的是，每个acquire()调用都必须有一个对应的release()调用。

条件锁（Condition）：threading.Condition()函数返回一个条件变量对象，可以用于线程之间的协调和通信。它提供了wait()、notify()和notify_all()等方法，用于线程的等待和唤醒。

信号量（Semaphore）：threading.Semaphore()函数返回一个信号量对象，可以控制同时访问某个资源的线程数量。它有一个内部计数器，每次调用acquire()方法时计数器减一，调用release()方法时计数器加一。

事件（Event）：threading.Event()函数返回一个事件对象，可以用于线程之间的通信和同步。它有一个内部标志，默认为False，可以使用set()方法设置为True，使用wait()方法等待事件变为True，使用clear()方法将事件标志重新设置为False。
